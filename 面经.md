

# 0. 决策树

## 0.1 ID3

### 0.1.1 思想

- 信息熵越大，样本纯度越低；ID3的核心就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂；

- 信息增益即为得知特征A的信息从而使得不确定性减少的程度；

- 信息增益=信息熵-条件熵；

  **数据集的信息熵**

  ​	
  $$
  H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
  $$

  - 其中，$C_k$表示集合D中属于第k类样本的样本子集；	
  
  **针对某个特征A，对于数据集D的条件熵H(D|A)**为：

$$
H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{D}H(D_i)\\\
=-\sum_{i=1}^n\frac{|D_i|}{|D|}(\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|})
$$

​		-  其中，$D_i$表示D中特征A取第i个值的样本子集，$D_{ik}$表示$D_i$中属于第k类的样本子集

### 0.1.2 思想

- 初始化特征和数据集合；
- 计算数据集的信息熵和所有特征的条件熵，选择信息增益最大的特征进行分裂；
- 更新特征和数据集合（删除上一步使用的特征，并按照特征值来划分不同分支的集合；
- 重复2、3步，若子集包含单一特征，则为分支叶子节点；

### 0.1.3 缺点

- ID3没有剪枝策略，容易过拟合；
- 对取值较多的特征有偏好，ID的特征信息增益接近于1；
- 只能处理离散分布的特征；
- 没有考虑缺失值；

## 0.2 C4.5

- 引入悲观剪枝策略进行后剪枝；

- 引入信息增益率（信息增益/特征信息熵）；
  $$
  Gain_{ratio}(D,A)=\frac{Gain(D,A)}{H_A(D)}\\
  H(A,D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
  $$
  Note: 信息增益率对取值较少的特征有所偏好，分母越小，整体越大，因此C4.5并不直接使用增益率最大的特征进行划分，而是使用一个**启发式方法**：先从候选特征中选择信息增益高于平均值的特征，在从中选择增益率最高的；

- 连续特征离散化，n个样本，对于某个连续特征A，有m个取值，将样本按照特征进行排序并取相邻两样本值的平均数共m-1个划分点，分别计算以该划分点作为二元分裂点时的信息增益，并取信息增益最大的点作为二元离散分裂点；

- 特征值缺失下如何进行特征选择：
  
  - 对于具有缺失值特征，用没有缺失的样本子集计算信息增益率；
  
- 选定该缺失特征时，对于缺失该特征的样本如何处理（划分到哪个节点）：
  
- 一定概率划分到不同节点；
  
- 预剪枝：节点划分前进行，方法有：
  - 节点内样本数量小于一定阈值；
  - 所有节点特征都已分裂；
  - 节点划分前准确率比划分高；
  
- 后剪枝：划分后进行：
  
  - C4.5采用悲观剪枝方法，a)  递归的方式从下往上针对每一个非叶子节点，评估用一个最佳叶子节点代替这颗子树是否有增益（C4.5通过训练集的错误分类数量来估算未知样本的错误率）；
  
- 缺点：
  - 多叉树；
  - 只能处理分类任务；
  - 熵计算具有大量的对数运算；

## 0.3 CART

### 0.3.1 思想

- 分裂：二叉递归划分过程，输入和预测特征可连续可以离散，没有停止准则，会一直生长下去；
- 剪枝：代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能影响最小的那个分裂点作为下一个剪枝对象，直到只剩下根节点，CART会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；
- 树选择：用单独的dev集或者test集评估；

### 0.3.2 改进

- C4.5多叉树，cart树二叉，运算快；

-  C4.5分类，cart既分类又回归；

-   cart用基尼系数，减少了大量耗时的对数运算；

-    cart基于复杂度剪枝，c4.5采用悲观剪枝；

-   基尼系数可以理解为熵运算的一阶泰勒展开；

### 0.3.3 基尼系数计算

![image-20220515123024194](/Users/lirui/Library/Application Support/typora-user-images/image-20220515123024194.png)

- 基尼系数反映了从数据集随机抽取两个样本，标记不一致的概率；（基尼系数越小，数据的纯度越高）

### 0.3.4 缺失处理

划分特征时，对于缺失值的样本如何处理？

-  为每个树的节点都找到代理分裂器，在划分缺失特征的样本时，采用排名最高的代理来决定；

### 0.3.5 CART树剪枝

从生成算法产生的决策树T0底端开始不断剪枝，，直到T0的根节点，形成一个子树序列；然后通过交叉验证法在独立的验证集上对子树序列进行预测，选择最优子树。（剪枝的过程中通过一个$\alpha$参数来控制训练数据的拟合程度和树的复杂度，评判将树节点替换为叶子时损失函数减少的程度）;

### 0.3.6 CART回归树

\-  连续值处理：采用和方差划分特征，对于任意划分特征A，对应的划分点S两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2均方差和最小的特征作为划分；

\-  预测：分类树采用叶子节点概率最大的类别作为当前节点的预测类别，回归树采用叶子的均值或中位数来预测输出结果；

![image-20220515122930809](/Users/lirui/Library/Application Support/typora-user-images/image-20220515122930809.png)

![image-20220515122944817](/Users/lirui/Library/Application Support/typora-user-images/image-20220515122944817.png)

![image-20220515122952564](/Users/lirui/Library/Application Support/typora-user-images/image-20220515122952564.png)

## 0.4 总结

- **划分标准的差异**：ID3使用信息增益偏向取值较多的特征，C4.5使用信息增益率来克服信息增益的缺点，偏向于特征值小的特征，CART使用基尼系数克服C4.5需要大量的对数运算，但偏向于特征值较多的特征；
- **样本数据差异：**ID3只能处理离散数据且对缺失值敏感，C4.5和CART可以处理连续数据且有多种方式处理缺失值
- **使用场景差异**：ID3和C4.5都只能处理分类问题，CART既可以分类又可以回归，ID3和C4.5是多叉树，CART是二叉树；
- **样本特征差异**：ID3和C4.5层级间只使用一次特征，CART可以多次重复使用特征；
- **剪枝策略**：ID3没有剪枝，C4.5悲观剪枝，CART代价复杂度剪枝；

# 1. GBDT

## 1.1 原理

- GBDT通过多轮迭代，每轮迭代产生一个弱分类器，每个弱分类器在上一个分类器的残差(**拟合负梯度)**的基础上进行训练；

- 弱分类器的要求一般是足够简单，低方差且高偏差，因为boosting训练过程中是通过降低偏差来不断提高最终分类器的精度（选为CART回归树）；

- 最终的总分类器是每轮训练的弱分类器加权求和得到的；
  $$
  F_m(x)= \sum_{m=1}^{M}T(x;\theta_m)
  $$
  -模型一共训练M轮，每轮产生一个弱分类器$T(x;\theta_m)$;

- 当损失函数为MSE时，拟合残差；

## 1.2 用于分类

- GBDT用于分类or回归时，所用的都为CART回归树；

- 对于多分类任务，针对样本每个可能的类，都训练一个回归树；

- 假设为三分类任务，设样本x属于第二类，那用一个一维向量来表示样本x的类别[0, 1, 0]

- 训练时生成三颗回归树，第一颗树针对样本x的第一类，输入为(x,0)，第二颗树针对样本x的第二类，输入为(x,1)，第三颗树输入为(x,0)，第1次迭代时对样本x类别的预测值为$f_1(x)$，$f_2(x)$，$f_3(x)$，利用softmax来生成预测概率，属于类别1的概率为：
  $$
  p_1=exp(f_1(x))/\sum_{k=1}^{3}exp(f_k(x))
  $$

- 针对第一颗树，残差为$y_{11}(x)=0-p_1(x)$，则在第二轮迭代时输入树为($x$, $y_{11}(x)$)，反复;

## 1.3 多分类示例

[GDBT多分类]: https://www.cnblogs.com/modifyrong/p/7744987.html



# 2. XGBoost

## 2.1 原理

### 2.1.1 目标函数

- XGBoost是由k个基模型组成的一个加法运算式：

$$
\hat{y}_{i}={\sum_{t=1}^{k}}f_t(x_i)
$$

- 目标函数由损失函数和正则项组成：
  $$
  Obj ={\sum_{i=1}^{n}}l(y_i, \hat{y_i})+{\sum_{t=1}^{k}}\Omega(f_t)
  $$

- boosting模型都是前向加法，第t步的模型为例，模型对第i个样本的预测为：
  $$
  \hat{y_i}^t=\hat{y_i}^{t-1}+f_t(x_i)
  $$

- 因此，目标函数可以写为：
  $$
  Obj={\sum_{i=1}^{n}}l(y_i, \hat{y_i}^{t-1}+f_t(x_i))+{\sum_{t=1}^{k}}\Omega(f_t)
  $$

- 对目标函数进行二阶泰勒展开：
  $$
  Obj^{(t)}={\sum_{i=1}^{n}}[l(y_i, \hat{y_i}^{t-1})+g_if_t(x_i)+1/2h_if^2_t(x_i)]+{\sum_{i=1}^{t}}\Omega(f_t)
  $$
  -其中，**g为损失函数一阶导数，h为损失函数二阶导，这里的导数是对$\hat{y_i}^{t-1}$求导**

- 以平方损失函数为例
  $$
  g_i=2(\hat{y_i}^{t-1}-y_i)
  $$

  $$
  h_i=2
  $$

- 由于在第t步时t-1步的预测值已经为已知的，因此目标函数可以改写为：
  $$
  Obj^{(t)}={\sum_{i=1}^{n}}[g_if_t(x_i)+1/2h_if^2_t(x_i)]+{\sum_{i=1}^{t}}\Omega(f_t)
  $$

- 因此，只需要求出每一步的损失函数的一阶导和二阶导，然后最优化目标函数，就可以得到每一步的f(x)，之后根据加法模型得到一个整体模型

### 2.1.2 基于决策树的目标函数

- 决策树的复杂度可以由叶子数T组成，叶子节点越少模型越简单，此外叶子节点也不应该有过高的权重w，目标函数的正则化项定义为：
  $$
  \Omega(f_t)=\gamma T+1/2\lambda \sum_{j=1}^{T}w^2_j
  $$

- 设$I_j$为第j个叶子节点的样本集合，故目标函数可以写为：
  $$
  Obj^{(t)} \approx \sum_{i=1}^{n}{[g_if_t(x_i)+1/2h_if^2_t(x_i)]}+\Omega(f_t)\\
  =\sum_{i=1}^{n}[g_iw_{q(x_i)}+1/2h_iw^2_{g(x_i)}]+\gamma T+1/2\lambda \sum_{j=1}^{T}w^2_j\\
  =\sum_{j=1}^{T}[(\sum_{j\in I_j}{g_i})w_j+1/2(\sum_{j\in I_j}h_i+\lambda)w^2_j]+\lambda T
  $$

  - 第二步是遍历所有样本后取每个样本的损失函数，但样本最终会落在叶子节点上，因此可以遍历叶子节点，然后获取叶子节点的样本集合，在求损失函数(T为叶子数量)
  - 简化表达式，定义$G_j=\sum_{j \in I_j}g_i$，$H_j=\sum_{j \in I_j}h_i$，目标函数写为：

  $$
  Obj^{(t)}=\sum_{j=1}^{T}[G_j w_j+1/2(H_j+\lambda)w^2_j]+\lambda T
  $$

- 注意这里$G_j$和$H_j$是前t-1步的结果，因此值视为常数，只有最后一颗树的叶子节点$w_j$不确定，将目标函数对$w_j$求一阶导并令其为0，得到叶子节点相应的权值为：
  $$
  w_j^*=-G_j/(H_j+\lambda)
  $$

- 目标函数简化为：
  $$
  Obj^{(t)}=-1/2\sum_{j=1}^{T}G_j^2/(H_j+\lambda)+\gamma T
  $$
  ![目标函数](/Users/lirui/Desktop/面经/xgboost1.png)

### 2.1.3 最优切分点划分算法

- XGBoost支持两种分裂节点的方法，贪心算法和近似算法

#### 1）贪心算法

- 从深度为0的树开始，对每个叶子节点枚举所有的可用特征；

- 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；

- 选择收益最大的特征作为分裂特征， 用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶子节点，并为每个新节点关联对应的样本集合；

- 重复第一步，递归执行直到满足条件；

  如何计算特征的分裂增益？

  假设在某一节点完成分裂，分列前的OBJ可以写为：
  $$
  Obj_1 = -\frac{1}{2}[\frac{(G_L+G_R)^2}{(H_L+H_R+\lambda)}]+\gamma
  $$
  分裂后的目标函数为：
  $$
  Obj_2 = \frac{1}{2}[\frac{(G_L)^2}{(H_L+\lambda)}+\frac{(G_R)^2}{(H_R+\lambda)}]+2\gamma
  $$
  分裂后的增益为：
  $$
  Gain=\frac{1}{2}[\frac{(G_L)^2}{(H_L+\lambda)}+\frac{(G_R)^2}{(H_R+\lambda)}-\frac{(G_L+G_R)^2}{(H_L+H_R+\lambda)}]-\gamma
  $$
  对于每次分类，要枚举所有特征可能的分隔方案，假设要枚举所有x<a的条件，只要做一遍从做到右的扫描即可枚举出所有分割的梯度和$G_L$和$G_R$;

#### 2）近似算法

- 近似算法主要针对贪婪算法需要将特征全部读入内存这一限制进行的优化，可以得到近优解；
- 对于每个特征，只考虑分位点

### 2.1.4 稀疏感知算法

- XGBoost在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方法上，最优的缺省方向可以从数据中学到（即分别枚举特征缺失的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向）

## 2.2 工程优化

- **并行**：boosting为串行无法优化，但对于每个决策树的特征分裂，作了并行化处理，对于不同的特征的特征划分点，在不同线程中并行选择分裂最大的增益；
- **内存优化**：对训练的每个特征排序并且以块的形式存储在内存中；
- **存储+IO**：设置合理的分块大小，充分利用CPU缓存进行读取加速；

## 2.3 优缺点

### 2.3.1 优点

- **精度更高**：GBDT只用一阶泰勒展开，XGBoost对损失函数用到了二阶泰勒展开；
- **灵活性增强**：GDBT以CART为基分类器，而XGBoost不仅支持CART还支持线性分类器，XGBoost还支持自定损失函数，只要确保损失函数可以进行一阶和二阶求导；
- **正则化**：目标函数加入了正则项，包括叶子节点数量、叶子节点权重的L2范数；
- **Shrinkage**：学习率，每次迭代后，将叶子节点的权重乘以该系数，为了削减每颗树的影响；
- **列抽样**: 借鉴随机森林的做法，特征列采样；
- **缺失处理**: 稀疏感知算法；
- **并行化**:块结构可以支持特征并行化计算；

### 2.3.2 缺点

- 利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但节点分裂过程中仍需遍历数据集；
- 预排序空间复杂度较高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引；

# 3. LightGBM

## 3.1 单边梯度抽样算法(GOSS)

### 3.1.1 单边梯度抽样算法(GOSS)

- GOSS算法保留了梯度小的样本，对梯度小的样本进行随机抽样，为了不改变样本的数据分布，计算增益时为梯度小的样本引入一个常数项进行平衡；
- GOSS事先基于梯度的绝对值对样本进行排序，然后拿到前%a梯度大的样本和总体样本的b%，计算增益时，通过乘(1-a)/b来放大梯度小的样本的权重。**一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘以权重来防止采样对原始数据分布造成太大的影响；**

### 3.1.2 直方图算法

#### 1）直方图算法

将连续特征离散化为k个离散特征，同时构造一个宽度为k的直方图用于统计信息

- 内存占用更小；
- 计算代价更小；·

#### 2）直方图加速

构造叶子节点的直方图时，还可以通过父节点的直方图与相邻叶子节点的直方图相减的方式构建，从而减少了一半的计算量。

#### 3）稀疏特征优化

只用非零值构建直方图

### 3.1.3 互斥捆绑特征算法

将一些特征进行绑定，可以有效降低特征数量

### 3.1.4 带深度限制的Leaf-wise算法

- Level-wise：基于层进行生长，直到达到停止条件；

- Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件

  ![xgboost2](/Users/lirui/Desktop/面经/xgboost2.png)

### 3.1.5 类别特征最优分割

- 决策树并不推荐one-hot编码
  - 产生样本切分不平衡问题：特征域取值较多时，在每个取值的分裂中，大量样本为0，少量样本为1，划分的增益很小；
  - 影响决策树学习：决策树依赖数据的统计信息，独热编码会把数据切分到零散的小空间，这些零散的小空间上统计信息不准确，学习效果变差（本质上因为在选择最优分裂点时，one-hot特征由于被切分到多份，每一份与其他特征竞争最优点时都失败，因此该特征的重要性会比实际值低）；
  - LGB对离散特征分裂时，每个取值都当做一个桶，分裂的增益算的是“是否属于某个category的gain”，类似one-hot;

## 3.2 工程优化

- 特征并行：不进行数据垂直划分，减少不必要的通信；
- 数据并行：采用分散规约的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价；
- 缓存优化

## 3.3 与XGBoost的对比

### 3.3.1 内存更小

- 直方图算法将特征转为bin值，且不需要记录特征->样本的索引，减少了内存消耗；
- 训练过程中的互斥特征捆绑算法减少了特征数量

### 3.3.2 速度更快

- 单边梯度算法过滤了梯度小的样本，减少了大量的计算；
- 基于leaf-wise的生长策略；
- 对特征并行和数据并行进行了优化；
- 对缓存也进行了优化，增加了cache hit的命中；

# 4. 协同过滤算法

## 4.1 Item-CF

- 给用户推荐他喜欢过物品的相似的物品；

- 相似度计算并不是基于物品的内容属性，而是分析用户的历史行为计算物品间的相似度；
  $$
  R_{u,p}=\sum_{h\in H}(w_{p,h}*R_{u,h})
  $$
  -$R_{u,p}$表示用户u对物品p的喜好程度，H为相似物品集合，$w_{p,h}$为物品p和h的相似度

```python
def get_sim_item(df, user_col, item_col, user_iff=False):
  # user_col: 用户列名 eg:"user_id"
  # item_col: item列名 eg: "item_id"
  # df: user和item交互数据
  # user_iff：是否采样iff计算
  user_item = df.groupby(user_col)
  user_item_dict = dict(zip(user_item[user_col], user_item[item_col]))
  
  sim_item = {}
  item_cnt = defaultdict(int)
  for user, items in user_item_dict.items():
    for it in items:
      item_cnt[it] += 1
      sim_item.setdefault(it, {})
      for relate_item in items:
        if it == relat_item:
          continue
        	sim_item[it].setdefault(relate_item, 0)
          if not use_iff:
            sim_item[it][relate_item] += 1
           else:
            sim_item[it][relate_item] += 1 / math.log(1 + len(items))
	sim_item_corr = sim_item.copy()
  for it, relat_it in sim_item.items():
    for j, cij in relat_it.items():
      sim_item_corr[i][j] = cij / math.sqrt(item_cnt[i] * item_cnt[j])
  
  return sim_item_corr, user_item_dict

def recommend(sim_item_corr, user_item_dict, user_id, top_k, item_num):
  # sim_item_corr: 物品相似度矩阵
  # user_item_dict: 用户历史交互项目矩阵
  # user_id: 待推荐用户id
  # top_k: 查询用户每个历史item相似的item数量
  # item_num: 最终返回的item 数量
  rank = {}
  interacted_items = user_item_dict[user_id]
  for i in interacted_items:
    for j, wj in sorted(sim_item_corr[i].items(), key=lambda d:d[1], reverse=True)[0:topk]:
    	if j not in interacted_items:
        rank.setdefault(j, 0)
        rank[j] += wj
  return sorted(rank.items(), key= lambda d:d[1], reverse=True)[:item_num]
```

- 代码逻辑：

  - 相似性计算：

    1）统计两商品的共现次数，即用户同时喜欢两个物品；

    2）通过Jacarrd/余弦相似度来计算两两商品的相似性；

    3）计算时对热门商品or热门用户做惩罚；

  - 推荐：

    1）获取待推荐用户的历史交互商品，在I2I相似度矩阵中获取每个item的最相似的topk个item，计算每个item的累积权重；

    2）若推荐物品少于需求，用热门填充；

## 4.2 User-CF

- 给用户推荐和他相似的用户喜欢的东西；

- 不适合用户量很大或频繁更新的场景
  $$
  R_{u,p}=\frac{\sum_{s\in S}(w_{u,s}*R_{s,p})}{\sum_{s\in S}w_{u,s}}
  $$

```python
def get_sim_user(df, user_col, item_col, use_iff=False):
  item_user = df.groupby(item_col)[user_col].agg(list).reset_index()
  item_user_dict = dict(zip(item_user[item_col], item_user[user_col]))
  
  user_item = df.groupby(user_col)[item_col].agg(list).reset_index()
  user_item_dict = dict(zip(user_item[user_col], user_item[item_col]))
  
  sim_user = {}
  user_cnt = defaultdict(int)
  for item, users in item_user_dict.items():
    for u in users:
      user_cnt[u] += 1
      sim_user.setdefault(u, {})
      for relate_user in users:
        if u == relate_user:
          continue
        sim_user[u].setdefault(relate_user, 0)
        if not use_iff:
          sim_user[u][relate_user] += 1
         else:
          sim_user[u][relate_user] += 1 / math.log(1 + len(users))
	sim_user_corr = sim_user.copy()
  for u, relate_u in sim_user_corr.items():
    for v, cij in relate_u.items():
      sim_user_corr[u][v] = cij / ((user_cnt[u] ** 0.5) * (user_cnt[v] ** 0.5)))
  
  return sim_user_corr, user_item_dict

def user_recommend(sim_user_corr, user_item_dict, user_id, topk, item_num):
  rank = {}
  related_users = sorted(sim_user_corr[user_id], key=lambda d: d[1], reverse=True)[0:topk]
  for u in enumerate(relate_users):
    for i in user_item_dict[u]:
      if i not in user_item_dict[user_id]:
        rank.setdefault(i, 0)
        rank[i] += sim_user_corr[user_id][u]
  return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]
```

# 5. 深度学习优化器

## 5.1 深度学习优化算法

### 5.1.1 梯度下降算法

- 利用一阶偏导数求解，当损失函数为凸函数时，梯度下降法得到的解为全局最优解；

  ![梯度下降算法](/Users/lirui/Desktop/面经/梯度下降算法.png)

- 算法的步长，初始值，及样本是否归一化均会影响收敛速度和收敛点；
- 与最小二乘的比较：
  - 梯度下降法需要选择步长，最小二乘法不需要；
  - 梯度下降法是迭代求解，最小二乘法是计算解析解；

### 5.1.2 牛顿法

- 一般用于求方程的根或最优化；

- 最优化求解:

  - 设$f(x)$具有二阶连续导数，第k次迭代值为$x^{(k)}$，将$f(x)$在$x^{(k)}$处进行二阶泰勒展开：
    $$
    f(x)=f(x^{(k)}) + g^T_k(x-x^{(k)})+\frac{1}{2}(x-x^{(k)})H(x^{(k)})(x-x^{(k)})
    $$

    - 其中，$g_k$为$f(x)$的梯度向量$x^{(k)}$处的值，$H(x^{(k)})$为$f(x)$的Hessian矩阵；

    $$
    H(x)=[\frac{\partial^2 f}{\partial x_i \partial x_j}]_{n\times n}
    $$

  - 函数有极值的条件是极值点处一阶导数为，对上述泰勒公式再次进行求导；
    $$
    \nabla f(x)=g_k+H_k(x-x^{(k)})
    $$

  - 即得到迭代公式：
    $$
    x^{(k+1)}=x^{(k)}- H^{-1}_kg_k
    $$

- 与梯度下降法相比，牛顿法是利用二阶Hessian矩阵的逆矩阵来进行求解，相对而言，牛顿法收敛更快（迭代次数变少），但单次求解时间变长；即牛顿法相当于用一个二次曲面来拟合当前的局部曲面，而梯度法是用一个平面来拟合；

### 5.1.3 拟牛顿法

- 牛顿法需要计算Hessian矩阵的逆计算复杂，因此用一个n阶矩阵$G_k$来近似替代$H_k^{(-1)}$，$G_k$要满足和$H_k$类似的性质；

- $G_k$应为正定，且满足下面的拟牛顿条件：
  $$
  G_{(k+1)}y_k=\delta k
  $$

  - $y_k$为$g_k-g_{k-1}$，$\delta k = x^{(k)}-x^{(k-1)}$;

## 5.2 优化器

### 5.2.1 BGD

- 采用整个训练集的数据来计算cost function对参数的梯度：
  $$
  \theta=\theta-\eta \nabla_{\theta}J(\theta)
  $$

- 对于凸函数，可以收敛到全局极小值，非凸函数可以收敛到局部极小值；

### 5.2.2 SGD

$$
\theta=\theta-\eta \nabla_{\theta}J(\theta; x^{(i)}, y^{(i)})
$$



- SGD一次只用一个样本进行梯度更新计算
- 缺点是SGD的噪音比较多，使得其每次迭代并不都向着最优化的方向，因此虽然速度变快，但并不是全局最优；
- 优点是速度快，可以在线更新；
- 因为SGD每一次迭代的梯度受抽样样本影响较大，因此学习率需要逐渐衰减，否则模型难收敛；

### 5.2.3 MBGD

$$
\theta=\theta-\eta \nabla_{\theta}J(\theta; x^{(i:i+n)}, y^{(i:i+n)})
$$

- 每次选用一小批样本来进行梯度更新；
- 容易陷入鞍点或局部极小值处震荡；
- 可以降低参数更新的方差，收敛更稳定；

**随机梯度下降算法的两大缺点：**

 <u>1）SGD容易困在鞍点或者局部极小值，或者在附近来回震荡；</u>

<u>2）SGD对所有参数更新时都应用同样的学习率，我们更希望对出现频率低的特征进行大一点更新，且LR会随着更新的次数逐渐变小；</u>

### 5.2.4 Momentum（针对缺点1）

- SGD更新时完全依赖于当前样本梯度，更新不稳定，在ravines的情况下容易被困住（ravines即曲面的一个方向比另一个方向更陡，这时SGD会发生震荡而不能解决极小值）；

- 即引入动量概念，在更新过程中一定程度上保留之前的更新方向，同时根据当前样本计算的梯度方向微调最终的更新方向；
  $$
  v_t=\gamma v_{t-1}+\alpha\nabla_{\theta}J(\Theta), \Theta=\Theta-v_t
  $$

  - 梯度方向变化时，momentum可以降低参数的更新速度，从而减少震荡；
  - 梯度方向相同时，momentum可以加速参数更新，加速收敛；

### 5.2.5 Adagrad（针对缺点2）

- 该算法可以对低频的参数做较大的更新，对高频的参数做较小的更新，因此其对稀疏的数据表现较好，且可以**自动调整学习率**；
  $$
  \theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\varepsilon }} \nabla_{\theta}J(\theta_i)
  $$

- 其中，$G_t \in R^{d\times d}$为对角矩阵，每个对角线位置i,i为对应参数$\theta_i$从第一轮到第t轮梯度的平方和，$\varepsilon$为平滑项，防止分母为0；
- 其缺点是在训练的中后期，分母上梯度平方的累积会越来越大，因此梯度趋近于0，学习率会最终变的很小；

### 5.2.6 RMSprop（针对缺点2）

- Adagrad会累加之前的梯度平方，而RMSProp仅仅计算对应的平均值，可以防止训练后期学习率下降过快的问题（AdaGrad每个变量的学习率在迭代过程中只能降低或者不变，因此很可能出现早期迭代到不好的极值点后，由于学习率太小而无法冲出）；

- 使用的是**指数平均**（旨在消除梯度下降中的摆动，与动量效果相同，某一维度的导数比较大，则指数加权平均就大，则相应的学习率变小，因此保证各维度导数都在一个量级，进而减少了抖动）；
  $$
  E[g^2]_t=0.9E[g^2]_{t-1}+0.1g^2_t\\
  \theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{E[g^2]_t+\varepsilon}}g_t
  $$

- 如果最近时间步的梯度平方加权项累积较小，说明梯度较小，那学习率会增加，如果最近时间步梯度的平方加权累积较大，说明梯度较大，那么学习率会减小；上述机制可以使得收敛稳定的同时，有一定几率冲出非优解；

### 5.2.7 Adam（针对缺点2）

- 利用梯度的一阶矩估计和二阶矩估计**动态调整每个参数的学习率**；

- 相当于**RMSprop+Momentum**；

  - 初始化：$m_t$=0，$v_t$=0;

  - 第t次迭代：

    - 在每个mini-batch中计算梯度$g_t$；

    - 计算梯度的指数衰减平均值和梯度平方的指数衰减平均值；
      $$
      m_t=\beta_1m_{t-1}+(1-\beta_1)g_t \\
      v_t=\beta_2v_{t-1}+(1-\beta_2)g^2_t
      $$

    - 因为$m_t$和$v_t$都被初始化为0，他们会向0偏置，因此做了**偏差校正**，来抵消偏差；
      $$
      \hat{m_t}=\frac{m_t}{1-\beta^t_1} \\
      \hat{v_t}=\frac{v_t}{1-\beta^t_2}
      $$

    - 梯度更新公式；
      $$
      \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_t+\varepsilon}}\hat{m}_t
      $$
  
- 参数设定值，一般$\beta_1=0.9$，$\beta_2=0.999$，$\varepsilon=10e-8$

## 5.3 优化器总结

- AdaGrad、RMSProp和Adam中，目标函数自变量的每个元素都分别拥有自己的学习率；
- AdaGrad目标函数自变量中各个元素的学习率只能保持下降或者不变，因此当学习率在迭代早期降的较快且当前解依然不佳时，由于后期学习率过小，可能难以找到一个比较有用的解；
- RMSProp和AdaDelta都是解决AdaGrad缺点的改进版本，本质思想都是利用最近的时间步的小批量随机梯度平方项的加权来降低学习率，从而使学习率不是单调递减；
- Adam可以视作RMSProp和动量法的结合，其相比RMSProp增加了bias-correction和momentum；
- 如果数据是稀疏的，采用学习率自适应的优化算法更好；
- SGD虽然也能到达极小值，但收敛速度更慢，且容易陷入鞍点；

# 6. PCA

## 6.1 PCA定义

- 通过线性投影将高维数据映射到低纬数据中，期望在投影的维度上，新特征自身的方差尽量大，方差越大特征越有效，尽量使产生的新特征间的相关性尽可能小；

## 6.2 证明

- 最大可分性；
- 最近重构性；

## 6.3 求解步骤

设有m条n维数据

1. 将原始数据按列组成n行m列的矩阵X；
2. 将X的每一行进行零均值化，即减去这一行的均值；
3. 求出协方差矩阵，$C=\frac{1}{m}XX^T$;
4. 求出协方差矩阵的特征值及对应的特征向量；
5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P；
6. $Y=PX$即为降维到k维后的数据；

## 6.4 性质

1. 缓解维度灾难：PCA算法可以通过舍去一部分信息后使得样本的采样密度增大；
2. 降噪：舍弃掉一部分小方差的特征一定程度上可以起到降噪的作用；
3. 过拟合：PCA保留了主要信息，但这部分信息可能是针对当前训练集的，因此有可能导致过拟合；
4. 特征独立：PCA使得降维后的数据各特征间相互独立；

## 6.5 细节

- 零均值化：对验证集和测试集进行同样降维时，均值必须要从训练集计算而来，防止分布偏差；
- 可以采用SVD进行求解；
- 降维的维度k可以根据交叉验证来选择；
- 若在做PCA前已知某几个变量高度相关，可以去掉，因为高度相关的变量可能导致特征的成分解释的方差变大；
- PCA的目的是降噪和去冗余：
  - 降噪是使得留下来的维度含有的能量尽可能大；
  - 去冗余是使得保留下的维度的相关性尽可能小；

## 6.6 降维的必要性

- 多重共线性变量-预测值相互关联，多重共线性可能会导致解空间不稳定，从而导致结果不连贯；
- 高维空间本身具有稀疏性；
- 过多变量会妨碍查找规律的建立；

## 6.7 PCA总结

- 优点：

  1）只需要使用方差来衡量信息量，不受数据集以外因素的影响；

  2）主成分间正交，可以消除变量共线性；

  3）计算效率高；

- 缺点：

  1）特征含义变模糊；

  2）方差小的非主成分可能也含有对样本区分的重要信息，因此降维丢弃可能对后续数据处理有影响；

# 7. 激活函数

## 7.1 Sigmoid

$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$

- 值域：(0, 1);
- 导数：$\sigma'(x)=\sigma(x)(1-\sigma(x))$
- 单调连续，处处可微，左右两侧都是近饱和区，存在梯度消失现象；
- 输出值不以0为中心，容易导致模型收敛速度变慢；
- 为了防止饱和，权重初始化不能过大，防止神经元饱和，梯度不更新；

## 7.2 Tanh

$$
Tanh(x)=\frac{e^{x}-e^{-x}}{e^x+e^{-x}}=2\cdot Sigmoid(2x)-1
$$

- 值域：[-1, 1]，单调连续，处处可微;
- 相比Sigmoid，收敛速度更快，因为0附近的线性区域内斜率更大，收敛速度会加快；
- 输出值以0为均值，也存在近似饱和区，且范围比Sigmoid更大；

## 7.3 ReLU

$$
ReLU(x)=\left\{\begin{matrix}
 x,   x>=0 & \\
 0,   x<0 &
\end{matrix}\right.
$$

- ReLU的输出仍然是非零对称的，可能出现dw恒为正或者恒为负，从而影响训练速度；
- 死区现象：x<0时，输出为0，反向传播梯度恒为0；
- 计算简单，没有指数运算，输入为正时无饱和区，可以解决梯度消失；

## 7.4 LeakyReLU

$$
ReLU(x)=\left\{\begin{matrix}
 x,   x>=0 & \\
 ax,   x<0 &
\end{matrix}\right.
$$

- 为负值定义了一个斜率，没有死区

## 7.5 P-ReLU

- 将LeakyReLU的超参数$\alpha$改为可学习的；

## 7.6 E-ReLU

$$
E-ReLU(x)=\left\{\begin{matrix}
 x,   x>=0 & \\
 \lambda (e^x-1),   x<0 &
\end{matrix}\right.
$$

- 能够使神经元的平均激活值趋于0；
- 对噪声具有更好的鲁棒性；

# 8. 概率和数理统计

## 8.1 概率

- 全概率公式：

  - 如果事件B1、B2、…、Bn构成一个完备的事件组，且其两两互不相容，和为全集，并且P(Bi)>0，则对一事件A有：

  ​										P(A)=P(A|B1)P(B1)+P(A|B2)P(B2)+…+P(A|Bn)P(Bn)，

  - 对于任意两随机事件A,B，如下成立：

  ​									
  $$
  P(B)=P(B|A)P(A)+P(B|\bar{A})P(\bar{A})
  $$

- 先验概率：

  根据以往经验和分析得到的概率（投骰子任意一面朝上的概率为1/6）

- 后验概率：

  事情已经发生，要求解这个事件发生的原因是由某个因素引起的可能性的大小；

  ![全概率](/Users/lirui/Desktop/面经/全概率.png)

- 频率学派与贝叶斯学派

  - 频率学派认为世界是确定的，有一个本体，本体的真值是不变的，我们的目标是找到这个真值或这个真值所在的范围；

  - 贝叶斯派认为世界是不确定的，人们对世界有一个预判，之后通过观测数据对预判做调整，目的是找到这个世界概率分布的最优表达；

    **区别：**

    ​	1） 基于总体信息和样本信息进行统计推断的理论称为古典统计学，基于总体信息、样本信息和先验信息进行统计推断的理论则为贝叶斯统计学；

    ​	2）频率学派坚持概率的频率解释，概率学派引入了主观概率；

    ​	3）频率学派把未知参数$\theta$看做一个未知的固定量，仅把样本看做随机量，而贝叶斯派则把未知参数也看做随机变量;

- 最大似然估计
  - 核心思想：找到参数$\theta$的一个估计值，使得当前样本出现的可能性最大；
  - 步骤：
    - 写似然函数；
    - 对似然函数求对数；
    - 对似然函数求导，令导数为0，求出的参数估计值则为最大似然估计；

- 贝叶斯估计
  $$
  p(y_i|x)=\frac{p(x_i,y_i)}{p(x_i)}=\frac{p(y_i)p(x|y_i)}{p(x)}
  $$

  - 其中，$p(y_i|x)$是后验概率，$p(x|y_i)$是条件概率，或者说似然概率

- 两者的区别：

  1）估计的参数不同，MLE要估计的参数被当做固定形式的一个未知变量，然后结合真实数据通过最大化似然函数求解这个固定形式的未知变量；贝叶斯估计则是将参数视为某种已知先验分布的随机变量，通过贝叶斯规则将参数的**先验分布转换为后验概率**求解；

  2）MLE为点估计，MAP是对分布估计；

## 8.2 概率和数理面试题

### 8.2.1 损失函数不可导方案

- 损失函数不可导时，可以采用**坐标轴下降法**，梯度下降是沿负梯度的方向进行，坐标轴下降是沿坐标轴方向。假设有m个特征，参数更新时，先固定m-1个，求这个参数的局部最优，使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果；

### 8.2.2 a,b$\sim$U[0,1]，互相独立，求Max(a,b)期望

![求期望](/Users/lirui/Desktop/面经/求期望.png)

### 8.2.3 切比雪夫不等式

$$
P(|X-\mu|\ge k\sigma)\le \sigma^2, k>0
$$

### 8.2.4 一根绳子，随机切三段，可以组成一个三角形的概率有多大？

![绳子面经](/Users/lirui/Desktop/面经/绳子面经.png)

### 8.2.5 共轭先验分布

- 假设$\theta$为总体分布中的参数，$\theta$的先验密度函数为$\pi(\theta)$，而抽样计算的后验密度函数与$\pi(\theta)$有相同的函数形式，则称$\pi(\theta)$为$\theta$的共轭先验分布；

### 8.2.6 概率和似然的区别

- 概率是在给定参数$\theta$的情况下，样本的随机向量X=x的可能性，而似然表示的是给定样本X=x的情况下，参数为$\theta$真值的可能性。一般情况下，对随机变量的取值用概率表示，在非贝叶斯统计的情况下，参数为一个实数而非随机变量，用似然表示。

### 8.2.7 岭回归和Lasso

1）标准最小二乘化问题
$$
f(w)=\sum_{i=1}^{m}(y_i-x_i^{T}w)^2=(y-Xw)^T(y-Xw)
$$

- 求解$\hat{w}=(X^TX)^{-1}X^Ty$，有解的唯一条件是列满秩；

2）岭回归

- 即使X列满秩，但当数据存在共线性时，即相关性较大时，会使得标准最小二乘求解不稳定，因此在loss上加一个正则化系数（L2）

$$
f(w)=\sum_{i=1}^{m}(y_i-x_i^{T}w)^2+\lambda\sum_{i=1}^{n}w_i^2
$$

- 即岭回归系数用矩阵形式表示为：
  $$
  \hat{w}=(X^TX+\lambda I)^{-1}X^Ty
  $$

- 就是通过将$X^{T}X$加上一个单位矩阵使得矩阵变成非奇异矩阵并可以进行求逆运算；

3）LASSO

- LASSO(The Least Absolute Shrinkage and Selection Operator)是另一种缩减方法，将回归系数收缩在一定的区域内。LASSO的主要思想是构造一个一阶惩罚函数获得一个精炼的模型, **通过最终确定一些变量的系数为0进行特征筛选**。

$$
\sum_{i=1}^{n}|w_i| \le t
$$

4) 最小二乘和极大似然

最小二乘法即为最小化平方和。

线性回归中，随机变量Y与x的关系是非确定性的，对于x的每一个取值，Y还受到一个随机因素的影响，从而有自己的分布。用F(Y|x)表示当x取一个确定值时，所对应的Y的分布函数。Y的期望E(Y)随x取值而定，是一个关于x的函数$\mu(x)$.

若$\mu (x)$为线性函数：$\mu (x)$=ax+b，若误差服从正态分布，即$\varepsilon \sim N(0, \sigma^2)$，相当于
$$
Y=ax+b+\varepsilon,\varepsilon \sim N(0, \sigma^2)
$$
即此时y也服从正态分布

![最小二乘vs极大似然](/Users/lirui/Desktop/面经/最小二乘vs极大似然.png)

### 8.2.8 l1正则化和l2正则化

1. 基于约束条件的最优化
   - l1正则化等价于在原优化目标函数中加入约束条件，$||w_1|| \le C$
   - l2正则化等价于在原优化目标函数中加入约束条件，$||w_1||^2_2 \le C$

2. 正则化理解之最大后验概率估计

   - l1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出；
   - l2正则化可以通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出；

3. 效果

    ![[公式]](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的效果是对原最优解的每个元素进行不同比例的放缩； ![[公式]](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化则会使原最优解的元素产生不同量的偏移，并使某些元素为0，从而产生稀疏性。

   [https://zhuanlan.zhihu.com/p/29360425](https://zhuanlan.zhihu.com/p/29360425)

# 9. 机器学习基础

## 9.1 相对熵

设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵为：
$$
D(p||q)=\sum_{x}p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
$$
一定程度上，相对熵可以衡量两个随机变量的距离，且D(p||q)不等于D(q||p)。

## 9.2 逻辑回归

![逻辑回归](/Users/lirui/Desktop/面经/逻辑回归.png)

## 9.3 SVM

## 9.4 朴素贝叶斯和LR

1）朴素贝叶斯是生成模型，LR是判别模型。朴素贝叶斯利用已有样本估计学习先验概率P(Y)和条件概率P(X|Y)，进而求出联合概率分布P(X, Y)，最后利用贝叶斯概率求后验概率P(Y|X)。

2）LR是利用极大似然概率求解P(Y|X)；

3）朴素贝叶斯具有很强的条件独立假设，即已知分类的Y的条件下，各个特征变量取值是互相独立的；

4）朴素贝叶斯适用于小数据，LR适用于大数据；

## 9.5 LR和线性回归

- 线性回归做回归任务，LR做分类任务；
- 线性回归用最小二乘求解，LR用最大似然估计求解；
- 线性回归受异常值影响大，LR受异常值影响小；

## 9.6 生成模型和判别模型

- 生成式：朴素贝叶斯、HMM、马尔科夫随机场；
- 判别式：LR、SVM、NN、GBDT
- 生成模型对联合概率建模，判别模型对条件概率建模；

## 9.7 交叉熵、信息熵和相对熵

- 信息熵：
  $$
  H(X)=-\sum_{x}p(x)log(p(x))=-\sum_{i=1}^np(x_i)log(p(x_i))
  $$
  $H(X)$被称为随机变量$X$的熵，表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量。从公式可知，随机变量的取值个数越多，状态数也就越多，信息熵也就越大，混乱程度就越大。当随机变量服从均匀分布时，熵最大；

- 条件熵：

  条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，条件熵定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望；
  $$
  H(Y|X)=\sum_{x}p(x)H(Y|X=x)\\
  =-\sum_xp(x)\sum_yp(y|x)logp(y|x) \\
  =-\sum_x\sum_yp(x,y)logp(y|x)
  =-\sum_{x,y}p(x,y)logp(y|x)
  $$
  条件熵$H(Y|X)$相当于联合熵$H(X,Y)$减去单独的熵$H(X)$

- 相对熵

  相对熵可以用来衡量两个概率分布的差异，下列公式的意义是求p与q间的对数差在p上的期望值，设p(x)、q(x)是离散随机变量X中取值的两个概率分布，则p对q的相对熵为：
  $$
  D_{KL}(p||q)=\sum_{x}p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
  $$

- 交叉熵
  $$
  H(p,q)=\sum_{x}p(x)log\frac{1}{q(x)}=-\sum_xp(x)log(q(x))
  $$

  - 即$D_{KL}(p||q)=H(p,q)-H(p)$，当用非真实分布q(x)得到的平均码长比真实分布p(x)得到的平均码长多出的比特数就是相对熵；

- 信息熵/交叉熵/相对熵的理解

  a)  信息论角度理解：

  -  信息熵：编码完美时，最短的平均码长是多少；
  - 交叉熵：编码不一定完美时，平均编码长度；
  - 相对熵：编码不一定完美时，平均编码长度相对于最小平均码长的增加量；

  b)  从熵角度理解 

  - 信息熵：根据真实分布，找到一个最优策略，以最小代价消除系统的不确定性，这个代价大小就是信息熵；
  - 交叉熵：衡量在给定的真实分布下，使用真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小；
  -  相对熵：衡量两个取值为正的函数或概率分布间的差异；

  3）最小化交叉熵等于最大化似然概率；

## 9.8 Kmeans

### 9.8.1 定义

- 把所有项分为k个簇，使得相同簇的所有项彼此尽量相似，而不同簇的项尽量不同。

### 9.8.2 流程

- 随机生成k个初始质心；
- 对于每一个样本计算其与各个聚类中心的距离，将样本划分与距离最近的聚类中心关联；
- 在新的聚类中，更新聚类中心；
- 重复步骤2、3，直至收敛或者到最大迭代次数

### 9.8.3 目标

- 使得总体群内方差最小，或平方误差函数最小

$$
J=\sum_{j=1}^k\sum_{i=1}^{n}||x_i^{(j)}-c_j||
$$

- 其中， k为簇个数，n为簇内样本数量

### 9.8.4 缺点

​	1）kmeans是局部最优的，容易受到初始质心的影响；

​	2） k值的选取也会影响聚类结果

​	3） 不能发现非凸形状的簇

​	4）对噪声和离群点敏感

​	5）算法时间复杂度较高，o(nkt)

### 9.8.5 距离

- 闵可夫斯基距离
  $$
  dist(X,Y)=\sqrt[p]{(\sum_{i=1}^n|x_i-y_i|^p)}
  $$

  - p=1时为曼哈顿距离；
  - p=2时为欧式距离；
  - p为无穷大时为切比雪夫距离；

- 余弦相似性距离
  $$
  dist(X,Y)=1-cos(\theta)
  $$

  - 当向量模值归一化时，欧式距离与余弦距离等价；

- KL散度（相对熵）

  - 非严格定义的距离，不满足三角不等式；

- Jacarrd相似性
  $$
  J(A,B)=\frac{|A\bigcap B|}{|A\bigcup B|}
  $$

- Pearson相关系数

  ![image-20220427233457180](/Users/lirui/Library/Application Support/typora-user-images/image-20220427233457180.png)

### 9.8.6 中止条件

- 迭代次数大于设置值，最小平方误差MSE小于阈值，或簇中心变化率小于阈值

## 9.9 密度聚类方法

只要样本点的密度大于某个阈值，则将该样本添加到最近的簇中，这类算法可以克服基于距离的算法中只能发现凸聚类的缺点，可以发现任意形状的聚类，对噪声不敏感。

1）DBSCAN

2）密度最大值算法

## 9.10 KNN

### 9.10.1 步骤

1. 算距离：给定未知样本，计算他与训练集中每一个样本的距离；
2.  找近邻：找到与未知对象距离最近的k个训练样本；
3. 做分类/回归：k个近邻中出现次数最多的类别作为未知对象的预测类别，或是取k个近邻的目标值平均数，作为未知对象的预测结果；
4. 当训练集类别数量不均衡时，解决方案即是同时考虑k个近邻的距离，根据距离的倒数来进行加权。

### 9.10.2 k值影响

- 一般情况下，k值越大，噪声影响越小；但k值过大，容易模糊边界（欠拟合）；

## 9.11 过拟合解决

1）早停； 2）l1和l2正则化；3）nn的dropout；4）决策树剪枝；5）SVM的松弛变量；6）集成学习bagging

## 9.12 分类问题为什么用交叉熵而不用MSE

1）损失函数角度

交叉熵：
$$
L=-(y_1log\hat{y}_1+...+y_klog\hat{y}_k)=-y_plog\hat{y}_p=\log\hat{y}_p
$$
均方误差损失：
$$
L=(y_1-\hat{y}_1)^2+(y_2-\hat{y}_2)^2+...+(y_k-\hat{y}_k)^2\\
=(1-\hat{y}_p)^2+(\hat{y}^2_1+\hat{y}^2_2+...+\hat{y}^2_k)
$$

- 交叉熵只与类别有关，$\hat{y}_p$越接近1越好；
- 均方误差不仅与$\hat{y}_p$有关，还与其他项有关，它希望$\hat{y}_1$、$\hat{y}_2$、...、$\hat{y}_k$越平均越好；

然而在分类问题中，对于类别中的相关性，我们缺乏先验。然而使用均方误差做损失函数会有倾向性问题，均方误差可能会给出错误的指示，即均方误差中label平均比有倾向性更好，对于交叉熵，损失函数只取决于label类别，更合理。

2）SoftMax函数角度

![image-20220429000829206](/Users/lirui/Library/Application Support/typora-user-images/image-20220429000829206.png)

![image-20220429000858272](/Users/lirui/Library/Application Support/typora-user-images/image-20220429000858272.png)

3) sigmoid函数角度

- 反向传播时的梯度与sigmoid有关，有单独的一项乘积项为$\sigma(x)$，因此当预测值趋于0时，梯度会比较小，可能导致收敛变慢甚至梯度消失；

  ![image-20220430000652839](/Users/lirui/Library/Application Support/typora-user-images/image-20220430000652839.png)

- Sigmoid的交叉熵优化为凸优化，但是sigmoid的MSE优化为非凸问题;

  ![image-20220430000713020](/Users/lirui/Library/Application Support/typora-user-images/image-20220430000713020.png)

## 9.13 **神经网络参数初始化**

- 预训练初始化：在大规模数据上用预训练模型训练好的权重向量来初始化下游任务的模型，可以找到一个较优的初始点；
- 随机初始化：如果全部初始化为0，在神经网络第一次前向传播时所有隐层神经元的激活值相同，反向传播时权重更新也相同，导致隐层神经元没有区分性，出现“对称权重”现象，因此需要随机初始化；
- 固定值初始化，如偏置向量用固定值0初始化；

### 参数初始化的Glorot条件

优秀的初始化要保证以下两个条件

（1）各个层的激活值（激活函数后的输出）方差要保持一致，即
$$
\forall(i,j) : Var(h_i)=Var(h_j)
$$
​	(2) 各个层对状态Z的梯度的方差要保持一致，即
$$
\forall(i,j):Var(\frac{\partial loss}{\partial z_i} )= Var(\frac{\partial loss}{\partial z_j} )
$$

### 参数初始化的几点要求

（1）参数不能全部初始化为0，也不能全部初始化为同一个值，否则会出现对称权重现象；

（2）最好保证参数初始化的均值为0，正负交错，正负参数大致上数量相等；

（3）初始化参数不能过大或过小，参数太小容易导致特征在每层间逐渐缩小而难以产生作用，参数太大会导致数据在逐层放大而导致梯度消失；

（4）最好能满足Glorot条件

### **三种常用的随机初始化方案**

1）基于固定方差的随机初始化

- 高斯分布初始化：使用高斯分布来对每个参数随机初始化；
- 均匀分布初始化：在[-r, r]内采用均匀分布随机初始化；
  - 固定方差随机初始化的关键在于如何设置方差，方差太小，会导致神经元输出过小，经过多层信号消失了，且sigmoid可能丢失非线性；方差过大，Sigmoid梯度接近于0，导致梯度消失；

2）基于方差缩放的参数初始化

​	若一个神经元参数很多，则每个输入连接上的权重应该小一些，以免输出过大，导致梯度爆炸或消失。因此，需要尽量保证每个神经元的输入和输出方差一致，即根据神经元连接数量自适应调整初始化分布的方差，称为方差缩放。

- Xavier初始化；

- Kaiming初始化（He初始化）

  |  初始化方法  | 激活函数 |             均匀分布              |                 高斯分布                  |
  | :----------: | -------- | :-------------------------------: | :---------------------------------------: |
  | Xavier初始化 | Logistic | $r=4\sqrt{\frac{6}{M_{l-1}+M_l}}$ | $\sigma^2=16\times \frac{2}{M_{l-1}+M_l}$ |
  | Xavier初始化 | Tanh     | $r=\sqrt{\frac{6}{M_{l-1}+M_l}}$  |     $\sigma^2=\frac{2}{M_{l-1}+M_l}$      |
  |   He初始化   | ReLU     |   $r=\sqrt{\frac{6}{M_{l-1}}}$    |       $\sigma^2=\frac{2}{M_{l-1}}$        |

Xavier初始化的缺点：Xavier推导是基于几个假设的，一个是激活函数是线性的，这并不适用于ReLU、Sigmoid等非线性激活函数；另一个是输出值关于0对称；

在Xavier论文中，作者给出的Glorot条件是：**正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度方差保持不变；**

在He初始化的论文中，条件变为：**正向传播时，状态值方差保持不变；反向传播时，关于激活值的梯度方差保持不变；**

He初始化针对relu为激活函数的神经网络

3）正交初始化

- 假设一个L层的等宽线性网络（激活函数为恒等函数）为
  $$
  y=W^{(L)}W^{(L-1)}...W^{(1)}x
  $$
  其中，$W^{(l)}\in R^{M\times M}(1\le l\le L)$为神经网络第$l$层权重，那么反向传播中，误差项$\delta$的反向传播公式为$\delta^{l-1}=((W^{(l)})^{(T)}\delta^{(l)})$。为了避免梯度消失或爆炸，希望误差项在反向传播中具有范数保存性，即$||\delta^{(l-1)}||^2=||\delta^{(l)}||=||(	W^{(l)})^{T}\delta^{(l)}||^2$，如果我们以均值为0、方差为$\frac{1}{M}$的高斯分布来随机生成权重矩阵中的每个元素的初始值，当$M\rightarrow \infty$，范数保持性成立，但是当不足够大是，这种对每个参数进行独立采样初始化的难以保证范数保持性。

  因此，一种更加直接的方式是将$W^{(l)}$初始化为正交矩阵，即$W^{(l)}(W^{(l)})^T=I$，这种方法称为正交初始化。这种方法称为正交初始化。正交初始化的具体实现过程可以分为两步：1）用矩阵为0、方差为1的高斯分布初始化一个矩阵；2）将这个矩阵用奇异值分解得到两个正交矩阵，并使用其中之一作为一个权重矩阵；

  **理想的参数初始化方案**

  经过多层网络后，信号不会产生梯度消失和梯度爆炸，即使每层网络输入和输出的方差一致，并且尽量每层网络的参数分布均值为0。

### 为何Logistic回归参数初始化可以都为0，而神经网络不行

[神经网络参数初始化](https://blog.csdn.net/weixin_43627748/article/details/113994231)

##  9.13 梯度消失和梯度爆炸

- 梯度消失：1）深层网络；2）损失函数不合适，比如Sigmoid；

- 梯度爆炸：1）深层网络；2）权重初始化过大；

  方案:

  1）预训练+微调；

  2）梯度裁剪+正则；

  3）换激活函数；

  4）BN；

  5）残差网络；

##  9.14 Label Encoding和One-Hot Encoding

- Label encoding将类别变量表示为有序编码，one-hot encoding将类别变量转化为无序编码，不同类别的编码向量彼此正交；
- 若模型为数值敏感性，即数值绝对大小对模型的训练有影响，如神经网络、SVM、LR，在编码时应该将类别特征编码成one-hot而不是label encoding；
- 若模型不为数值敏感性，即模型训练只与样本特征相对大小有关，例如树模型。当变量为有序变量时，采用label encoding编码，当变量为无序变量时，label encoding和one-hot差别不大；

##  9.15 损失函数

- 用于回归：MSE/RMSE/MAE/MAPE
- 用于分类：
  - 0/1损失函数，感知机使用；
  - 对数损失函数/交叉熵，分类任务；
  - 指数损失函数，Adaboost使用；$L(y,f(x))=exp(-y\cdot f(x))$
  - Hinge Loss: SVM；$L(Y,f(x))=max(0, 1-Y\cdot f(x))$

- Focal loss: 目标检测中，大量的样本其实贡献有限的loss，导致少量的正样本学习困难，所以增加这部分样本的权重；

  $FL=\left\{\begin{matrix}
   -\alpha(1-p)^{\gamma}log(p),   y=1 & \\
   -(1-\alpha)p^{\gamma}log(1-p),   y=0 &
  \end{matrix}\right.$

## 9.16  交叉熵函数与极大似然函数的区别与联系

- 区别：交叉熵用来描述预测分布与真实分布的差距，越小越好；似然函数用来衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。
- 联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，最小化交叉熵函数本质上等于对似然函数作最大化

## 9.17 SoftMax求导

![image-20220503001145545](/Users/lirui/Library/Application Support/typora-user-images/image-20220503001145545.png)

![image-20220503001153034](/Users/lirui/Library/Application Support/typora-user-images/image-20220503001153034.png)

## 9.18 朴素贝叶斯

### 9.18.1 分类公式

$$
p(类别|特征) = \frac{p(特征|类别)p(类别)}{p(特征)}
$$

### 9.18.2 原理

朴素贝叶斯是一个基于特征条件独立假设和贝叶斯原理的一种分类算法，朴素贝叶斯通过训练数据得到x与y的联合分布，之后对于要预测的x，根据贝叶斯公式，输出后验概率最大的y。

朴素贝叶斯是一种生成式模型，其生成方法是通过学习x和y联合分布实现的，加上各个特征在给定y的条件下相互独立。

### 9.18.3 假设

用于分类的特征在类确定的条件下条件独立。

### 9.18.4 优缺点

#### 优点

- 算法逻辑简单，易于实现；
- 分类速度快，准确度高；
- 对缺失数据不敏感，常用于文本分类；
- 对小规模数据表现很好，能处理多分类任务，适用增量训练；

#### 缺点

- 理论上，朴素贝叶斯相比其他分类方法有最小的误差率，但是实际并非如此，因为朴素贝叶斯假设在已知类别的情况下各属性间相互独立，这个假设在实际中往往不成立，因此在属性个数较多或者各个属性相关性较大的情况下，分类效果不好；
- 需要指定先验概率，且先验概率依赖于假设或者已有训练数据得到的，这在某些时候可能会因为假设先验概率的原因出现决策上的错误；

## 9.19 PageRank

### 9.19.1 Idea

pagerank表示在有向图上进行随机游走时，所形成的马尔科夫链的平稳分布，每个节点的PageRank值就是平稳概率，一旦网络的拓扑结构确定，PageRank值也确定；

### 9.19.2 随机游走模型

给定一个含有n个节点的有向图，在有向图上定义随机游走模型，即一阶马尔科夫链，转移矩阵是一个n阶矩阵M。
$$
M=[m_{ij}]_{n\times n}
$$
$m_{ij}$表示从节点$i$转移到节点$j$的概率。

### 9.19.3 PageRank的基本定义

给定一个包含n个节点的强连通且非周期性的有向图，在其基础上定义随机游走模型，即一阶马儿科夫链，这个马尔科夫链具有平稳分布R
$$
\lim_{t \to \infty}M^tR=R
$$
平稳分布R称为这个有向图的PageRank，R的各个分量称为各个节点的PageRank值。

### 9.19.4 PageRank的一般定义

w
$$
R=dMR+(1-d)/n +1
$$

## 9.20 BN

### 9.20.1 原因

解决internal covariate shift，保证数据分布的一致性。

### 9.20.2 作用

BN是一种正则化方法（减少泛化误差），其主要作用是：

1）加速网络训练

-  把输入分布拉回正态分布，使得通过激活函数前的输入值落在非线性函数对输入变化敏感的区域，避免梯度消失；
- 归一化每层和每维度的scale，可以使整体使用一个较高的学习率，而不必像以前那样迁就小scale的维度；

- 归一化后使得更多的分界面落在数据中，降低了overfit的可能性，因此一些防止过拟合但会降低速度的方法，例如dropout和权重衰减就可以不使用或者降低其权重；

2）缓解过拟合

- 同一个样本的输出不仅在取决于样本本身，也取决于跟这个样本属于同一个mini-batch的其他样本；

### 9.20.3 公式

$$
BN=\gamma*\frac{x-\mu}{\sqrt{\sigma+\epsilon}}+\beta
$$

### 9.20.4 原理

1）训练时，计算batch均值及方差，缩放因子$\gamma$和移位因子$\beta$；

2）推理时，均值和方差采用训练时的滑动平均；

### 9.20.5 细节

1）卷积层BN

​	设小批量中有 m个样本。在单个通道上，假设卷积计算输出的高和宽分别为p和q。需要对该通道中 m x p x q个元素同时做批量归一化。

2）BN两个参数的作用

用来恢复参数分布，BN在归一化时会破坏参数分布；

### 9.20.6 缺点

​	BatchSize大小对bn的影响较大。

### 9.20.7 和LN对比

LN是通道间归一化，BN是batchsize做归一化

1）在CV任务中，BN是只保留通道（channel）那一维度，LN保留Batchsize那一维度；

2）在NLP任务中，BN只保留句子的embedding维度，LN保留batchsize维度；

BN推理时用的均值和方差是训练集不同batch间的滑动平均，LN推理时用的均值和方差是自行计算的。

### 9.20.8 和Dropout同时使用的方差偏移![image-20220515003003381](/Users/lirui/Library/Application Support/typora-user-images/image-20220515003003381.png)

## 9.21 Dropout

### 9.21.1作用

防止过拟合，每次前向传播只让部分神经元参与，并且在反向传播时只让这部分神经元更新

### 9.21.2 为什么能防止过拟合

过拟合问题的本质是**部分神经元的权重太大**, 造成网络对输入数据十分**敏感**. 正常的神经网络训练很可能导致部分的神经元有相当大的表达能力. dropout的作用就是使模型中的每个神经元得到更加均匀和充分的训练, 使得模型整体的权重是一个非常小的值, 权重变小了, 模型对输入就不那么敏感了, **即使输入数据有微小变动, 输出数据也不会有太大变化。**

### 9.21.3 推理时用法

使用dropout测试时训练的权重需要除以1-p，保证训练测试时总体期望相同。

# 10. 数据预处理

##  10.1 缺失值处理

- 直接使用带有缺失值的数据（少数算法支持）；
- 删除缺失数据（信息损失，infer时可能分布不一致或者没见过该类数据）；
- 缺失值fillna；

##  10.2 缺失值填充方法

- 均值填充：浮点特征用平均数，离散特征用众数填充；
- 建模预测：将缺失特征作为label，建立模型预测；
- 高维映射：只针对离散特征，相当于增加一个新维度来存放缺失特征。
  - 思想：将属性映射到高维空间

![image-20220430103201160](/Users/lirui/Library/Application Support/typora-user-images/image-20220430103201160.png)

##  10.3 特征编码

- 特征二元化；
- one-hot；
  - 优点：
    - 能处理非数值特征；
    - 一定程度上也扩充了特征；
    - 编码后的属性是稀疏的；
  - 缺点：
    - 产生样本切分不平衡问题，切分增益会很小；
    - 决策树依赖的是原始数据的统计信息，而独热编码是将原始数据切分到几个零散的子空间上，在这些零散的子空间上，统计信息不准确，因此效果变差；
- 离散化：对连续特征进行分桶；

##  10.4 数据标准化

- 原因：

  1）某些算法要求数据为零均值和单位方差；

  2）可以消除样本不同属性量级差别过大的影响；

##  10.5 特征选择

- 子集搜索

  - 前向搜索/后向搜索；
  - 评价指标：特征方差/相关系数/卡方检验/互信息；

- 过滤式

  ![image-20220430104219626](/Users/lirui/Library/Application Support/typora-user-images/image-20220430104219626.png)

- 包裹式

![image-20220430104335023](/Users/lirui/Library/Application Support/typora-user-images/image-20220430104335023.png)

- 嵌入式

![image-20220430104359701](/Users/lirui/Library/Application Support/typora-user-images/image-20220430104359701.png)

##  10.6 类别不平衡问题

- 收集更多数据；
- 更换评价指标；
  - 通过混淆矩阵计算precision/recall；
  - F1-score；
  - Use Kappa（由类别不平衡程度normalized的一种准确度度量）；
  - ROC curves；
- data-level methods；
  - 基本思想是通过数据预处理改变数据的类别分布，将问题转化为平衡分类问题；
  - 过采样是通过向数据集中加入少数类样本来得到平衡的训练数据集（数据量少时用）；
  - 降采样是通过丢弃数据集的多数类样本来得到平衡训练数据集（数据量多时用）；
- alogrithm-level methods
  - 此类方法通过改变已有的学习算法来使其倾向于重视少数类数据的模式，这一过程可以通过加入数据权重或者对误分类数据的惩罚来使学习算法在不平衡数据上工作；
- hybrid methods
  - 混合类方法同时应用上述方法3、4来解决类别不平衡问题；

[https://zhuanlan.zhihu.com/p/54199094](https://zhuanlan.zhihu.com/p/54199094)

**data-level methods**

1) 过采样

- 随机过采样：
  - 首先在少数类$S_{min}$中随机选取一些少数类样本；
  - 然后通过复制所选样本生成样本集合；
  - 将它们添加至$S_{min}$中来扩大原始数据集从而得到新的少数类集合$S_{min-new}$;

- SMOTE算法：

  - 对于少数类中的每一个样本$x_i$，以欧式距离度量其到每个少数类样本集$S_{min}$中所有样本的距离，得到K近邻；

  - 根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其K近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$；

  - 对于每一个随机选出的近邻$\tilde{x}$，分别与原样本按照如下公式构建新样本；
    $$
    x_{new}=x+rand(0,1)\times (\tilde{x}-x)
    $$

- Bordline-SMOTE算法：

  - 对于每个$x_i\subset S_{min}$确定一系列K近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN} \subset S$;
  - 对每个样本，判断出最近邻样本集中属于多数类样本的个数，即$|S_{i-kNN} \cap S_{max}|$；
  - 最后，选择满足下面不等式的$x_i$：$\frac{k}{2} < |S_{i-kNN}\subset S_{max}| < k$，将其加入危险集DANGER；

  最后，对危险集中每一个样本点，采用SMOTE算法生成新的少数类样本；

2）欠采样

- 随机欠采样；

- Tomek links

  ![image-20220430231615877](/Users/lirui/Library/Application Support/typora-user-images/image-20220430231615877.png)

- NearMiss方法

  ![image-20220430231850554](/Users/lirui/Library/Application Support/typora-user-images/image-20220430231850554.png)

##  10.7 数据增强

### **Mixup**

#### 实现

从训练数据中随机抽取两个样本进行简单的随机加权求和，同时样本的标签也对应做加权求和。预测结果与对应加权求和的标签计算损失，反向传播更新参数。
$$
\tilde{x}=\lambda x_i+(1-\lambda)x_j\\
\tilde{y}=\lambda y_i + (1-\lambda)y_j
$$
参与训练的样本是由真实样本线性插值得到的；

#### 动机

大多数深度神经网络模型的训练旨在最小化模型在训练集上的平均误差是遵循**经验风险最小化(Empirical Risk Minimizatiom)**原则的。经验风险最小化表达的意思是: 因为我们不可能知道真实的数据分布情况(我的理解：现实情况数据量太大，可能性太多)，所以我们无法通过最小化模型的真实风险(最小化模型在真实数据集上的平均误差或者说损失函数)来训练模型，所以我们只能最小化模型在选定训练集上的经验风险(最小化模型在训练集上的平均误差或者说损失函数)。 这就导致了基于ERM(Empirical Rish Minimization)训练的模型展现出一些意想不到的行为：

1. 对训练样本的记忆：样本中存在的噪声样本会影响模型的性能
2. 对训练样本分布外的样本的敏感性：在面对对抗样本(adversarial examples)时性能很差

所以当测试数据集的分布稍微不同于训练数据集时模型不能提供很好的泛化能力，对训练数据过拟合。Mixup的提出就是想要减轻这些现象， 但是在讲到mixup之前我们要看一下另外一个概念：**领域风险最小化(Vicinal Risk minimization)**.

VRM(Vicinal Risk Minimization)原则跟ERM原则在思想上类似，核心不同点在于训练样本的构建。在ERM每一个原训练样本基础上，VRM需要人根据先验知识为每一个原样本描述一个领域，领域是由相似但又不同于原训练样本的样本组成的集合， 最后额外的样本从每一个原样本的领域中抽取出来来扩充整个训练样本集，这就是我们所说的data augumentation。在图像分类中领域的构建可以通过翻转， 旋转，缩放原样本。经过扩充的训练集可以提高模型的泛化能力， 但是如何通过data augmentation扩充数据集，和使用什么样的data augmentation方法是原训练集相关的和需要先验知识的。而且我们以上所提到的data augmentation方法所产生的新的样本都跟原样本属于同一个类别，并没有建模不同类别样本间的临近关系。 所以mixup作为一种简单的和数据集不相关的data augmentation方法在这种情况下被提出了。 Mixup足够简单，**通过对样本和标签进行线性插值就可以得到新的训练样本**，简单的几行代码不会对模型的训练带来太多额外的负担，但是可以很好的提高模型的泛化能力。

**mixup是对训练样本的各种有章法的变换，这就使得模型能够学到数据更加本质的特征，增强模型对样本细微变换的适应性（减弱对变化的敏感性）。**

# 11. 推荐算法面经

## 11.1 FM

$$
y(x)=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j
$$

## 11.2 FM的二阶优化

![image-20220430232705865](/Users/lirui/Library/Application Support/typora-user-images/image-20220430232705865.png)

![image-20220430232807942](/Users/lirui/Library/Application Support/typora-user-images/image-20220430232807942.png)

## 11.3 Embedding

在数学上表示为1个映射函数：f：X->Y，该函数满足两个性质：1）单射函数：每个Y只有一个X与其对应；2）structure-preserving：在X所属空间上$x_1<x_2$，则在Y所属空间上也有$y_1<y_2$

## 11.4 Wide & Deep

1）Wide是广义线性模型，增强模型的记忆能力

​	
$$
y=w^T[x,\phi(x)]+b
$$

- 其中，x和$\phi(x)$表示原始特征和叉乘特征；

2）Deep是深度模型
$$
a^{l+1}=f(W^la^l+b^l)
$$

- 其中，$W^l$、$a^l$和$b^l$是第l层的权重、输出和偏置；

3）记忆能力和泛化能力

- 记忆能力定义为了解物品和特征的共现频率，以及探索历史数据中的潜在关系例如，城市为北京用户的用户点击广告的概率为0.1，模型可以记住这个信息；
- 泛化能力基于现有联系，来探寻新的特征组合，可以类比为一种推理能力；

## 11.5 推荐去重（不推荐已经推荐过的item给用户）

BloomFilter算法

![image-20220430233936607](/Users/lirui/Library/Application Support/typora-user-images/image-20220430233936607.png)

## 11.6 SIM

1）movtivation

用户长期的兴趣很重要，MIND难以建立在给定target item下长期的序列行为兴趣

2）innovation

使用两个级联的搜索单元提取用户兴趣

-  General search unit作为从任意长原始序列的通用搜索，通过候选item的信息进行查询，获取相关的subuser behavior sequence；

- Exact search unit对候选item和SBS间的精确关系进行建模，这种级联搜索范式可以更好地在可扩展性和准确性方面对长期行为建模；

3）GSU

- Hard search: 非参数化的，根据类目来检索

- Soft search: 计算向量内积，选择topk

## 11.7 推荐系统中的偏差

### 11.7.1 显氏反馈中的偏差

1）选择偏差

​	当用户可以自由选择评分时，容易出现选择偏差，因为观察到的评分并不是所有评分的代表性样本

- 用户会选择他们喜欢的样本进行打分；
- 用户更倾向给特别好或者特别坏的商品打分；

2）一致性偏差

​	用户倾向于组内其他人评分相似，使得评分值可能并不是用户真正想法；

### 11.7.2 隐式反馈数据中的偏差

1）曝光偏差

​	用户只能接触到特定项目的一部分，因此未观察到的交互并不总是消极偏好；

2）位置偏差

​	用户倾向于位于推荐列表中**较高位置**的商品进行交互，而不管这些商品的实际相关性如何；

3）模型偏差

​	模型为了更好的学习目标函数而将其推广到训练数据之外作出的假设；

4）结果的偏差和unfairness

- 流行度偏差：推荐系统的马太效应；
- 不平衡：推荐系统不公平地歧视某些个人或个人群体而偏袒其他人；

### 11.7.3 反馈数据中的偏差解决

1. 选择偏差

   1）propensity score：用相反的倾向分数加权结果；

   2）ATOP：一种无偏估计的metric，有两个假设；a）相关（高）ratings在观察数据中随机缺失；b）对于其他rating值，允许以任意缺失机制，只要他们以高于相关rating的概率丢失；

   3）DATA imputation：对缺失数据做填补；

   4）以基于IPS的无偏估计为目标来优化loss；

   5）元学习

2. 一致性偏差

   1）将用户的评分作为用户喜好和social影响的综合结果

3. 曝光偏差

   1）实验IPS对于数据进行加权，把经常观测到的样本数据进行降权，而对少的样本进行升权处理；

   2）采样，均匀采样，流行负样本过采样；

   3）开发一个基于曝光的模型，预测商品曝光给用户的概率；

4. 位置偏差

   1）训练时将位置直接加入到特征中参与建模，infer时忽略；

   2）通过IPS来修正；

5. 流行度偏差

   1）正则；

   2）对热门商品进行加权修正；

   3）因果图；

## 11.8 推荐系统常用指标

### 11.8.1 混淆矩阵

- TP:正样本预测为正样本

- FP:负样本预测为正样本

-  FN:正样本预测为负样本

-  TN:负样本预测为负样本

### 11.8.2 准确率

​	预测正确的样本占总样本的比例

​	Accuracy = (TP+TN) / (TP+FP+FN+TN)

### 11.8.3 精确率

​	预测为正的样本中，正确预测为正样本的比例

​	Precision=TP/(TP+FP)

### 11.8.4 召回率

​	正确预测的正样本占实际正样本的比例

​	Recall = TP/(TP + FN)

### 11.8.5 F1 score

​	F1 = (Precision * Recall * 2) / (Precsion + Recall)

### 11.8.6 ROC & AUC

​	ROC的横坐标为FPR，纵坐标为TPR，FPR为真实的负样本中，预测为正的概率，TPR为真实的正样本中，预测为正的概率。

#### 混淆矩阵

![image-20220515102105317](/Users/lirui/Library/Application Support/typora-user-images/image-20220515102105317.png)

1）假阳性率：FPR = FP / (FP+TN) ，真实的负样本中，预测为正的比例；

2）真阳性率：TPR = TP/(TP+TN)，真实的正样本中，预测为正的比例；

#### ROC曲线

1）横坐标是假阳率，纵坐标是真阳率；

2）选取阈值来区分正负预测的结构，曲线恒过（0,0）和 (1,1);

#### AUC曲线

ROC曲线的面积，AUC衡量 了模型区分正负样本的能力，将正负样本间隔拉大的能力；

**即：随机从正样本和负样本中各选一个，分类器对于该正样本打分大于该负样本打分的概率；**

#### AUC计算公式

假设数据集一共有M个正样本，N个负样本，预测值即是M+N个，我们将所有样本按照预测值进行从小到大排序，并将排序编号由1到M+N；

1）对于正样本概率最大的，假设排序编号为$rank_1$，比它概率小的负样本个数为$rank_1-M$；

2）对于正样本概率第二大的，假设-排序编号为$rank_2$，比它概率小的负样本个数为$rank_2-M+1$;

...

3）对于正样本概率最小的，假设排序编号为$rank_M$，比它概率小的负样本个数为$rank_M-1$;

综上，在所有情况下，正样本打分大于负样本打分的个数为$(rank_1+rank_2+...+rank_M)-(1+2+...+M)$;
$$
AUC=\frac{\sum_{i\in POS}rank(i)-\frac{M\times(1+M)}{2}}{M\times N}
$$

- 对于预测值相同的样本，将原先的rank平均下，作为新的排序编号；

例：一堆样本，2000个负，4000个正，负样本的预测值为均匀分布U(0.4,0.6),正样本的预测分布为U(0.5,0.7),求AUC:

![image-20220515103216021](/Users/lirui/Library/Application Support/typora-user-images/image-20220515103216021.png)

#### AUC的优缺点

优点：1）AUC衡量的是一种排序能力，适合排序任务；

​			2）AUC对正负样本比例不敏感；

​			3）AUC不需要手动设定阈值；

缺点：1）无法反应实际的业务指标；

​			2）没有给出模型误差的空间分布信息，正样本内的排序信息不清晰；

#### AUC代码实现

```python
def cal_auc(y_pred, label):
	M = sum(label)
  N = len(label) - M
  f = list(zip(y_pred, label))
  rank = [val2 for (val1, val2) in sorted(f, key=lambda x: x[0])]
  pos_rank = [i + 1 for i in range(rank) if rank[i] == 1]
  return (sum(pos_rank) - M * (1 + M) / 2) / (M * N)
```



### 11.8.7 HIT@K

​	HR@K = NumberOfHits@K / GT

​	用于TOP-K推荐，推荐列表中前k个被真实点击的概率。

### 11.8.8 MRR (mean reciprocal rank)

[https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832](https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832)

正确检索的结果值在检索结果中的排名来估计检索系统的性能，只找出第一个相关的匹配项；

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220502235342088.png" alt="image-20220502235342088" style="zoom:50%;" />

 							<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220502235438583.png" alt="image-20220502235438583" style="zoom:50%;" />

### 11.8.9 MAP（mean average precision）

![image-20220502235629198](/Users/lirui/Library/Application Support/typora-user-images/image-20220502235629198.png)

![image-20220502235644678](/Users/lirui/Library/Application Support/typora-user-images/image-20220502235644678.png)

### 11.8.10 NDCG

![image-20220503000124583](/Users/lirui/Library/Application Support/typora-user-images/image-20220503000124583.png)

![image-20220503000134935](/Users/lirui/Library/Application Support/typora-user-images/image-20220503000134935.png)

## 11.9 Deep & Cross

https://zhuanlan.zhihu.com/p/55234968

# 12. NLP词向量

## 12.1 文本表示方法

- 基于one-hot、tfidf、textrank的方法；
- 主题模型：LSA、pLSA、LDA；
- 基于词向量的固定表征：Word2Vec、Fasttext、Glove；
- 基于词向量的动态表征：Elmo、GPT、Bert；

## 12.2 不同词向量特点

​	分布式假设指相同的上下文语境有相似的含义

1）one-hot表示：维度灾难、语义鸿沟；

2）分布式表示：

- 矩阵分解（LSA）：利用全局语料特征，但SVD计算复杂度较高；

- 基于NNLM和RNNLM的词向量：词向量为副产物，效率不高；

- Word2Vec和Fasttext：优化效率高，但是基于局部语料；

- Glove基于全局语料，结合了LSA和word2vec的优点；

- ELMO、GPT和BERT，动态表征；

  [https://www.cnblogs.com/sandwichnlp/p/11596848.html](NLP词向量)

## 12.3 基于one-hot编码的词向量方法

对词表的每个词进行编号，假设词表长度为n，则对于每个词的表征向量均为一个n维向量，且只在对应位置上的值为1，其他位置都为0；

![one-hot](/Users/lirui/Desktop/面经/one-hot.png)

One-hot将每个单词表示为完全独立的实体，但却会带来几个问题：

- 有序性问题：无法反映文本的有序性，因为语言并不是一个完全无序的随机序列。比如说，一个字之后只有接特定的字还能组成一个有意义的词，特定的一系列词按特定的顺序组合在一起才能组成一个有意义的句子；
- 语义鸿沟：其无法通过词向量来衡量相关词之间的距离关系，即这样的表征方法无法反映词之间的相似程度，因为任意两个向量的距离是相同的；
- 维度灾难：高维情形下将导致数据样本稀疏，距离计算困难，这对下游模型的负担是很重的；

基于此，出现了语言模型，最经典的便是N-gram模型和神经网络语言模型

## 12.4 统计语言模型

统计语言模型可以解决无序性这个问题。

**如何解决一段文本序列在某种语言下出现的概率？**

![image-20220501145039860](/Users/lirui/Library/Application Support/typora-user-images/image-20220501145039860.png)

## 12.5 分布式表征到SVD分解

### 12.5.1 分布式表征

![image-20220501145748109](/Users/lirui/Library/Application Support/typora-user-images/image-20220501145748109.png)

### 12.5.2 基于SVD的词向量方法

![image-20220501150148620](/Users/lirui/Library/Application Support/typora-user-images/image-20220501150148620.png)

![image-20220501150243891](/Users/lirui/Library/Application Support/typora-user-images/image-20220501150243891.png)

## 12.6 神经网络语言模型（NNLM）

由于N-gram模型的不足，2003年，Bengio发表了一篇文章：A neural probabilistic language model。该文章总结了一套用神经网络建立统计语言模型的框架（NNLM），并首次提出了word embedding的概念，NLP进入万物embedding的时代。

NNLM的基本思想：

- 假定词表中的每一个word都对应着一个连续的特征向量；

- 假定一个连续平滑的概率模型，输入一段词向量序列，可以输出这段序列的联合概率；

- 同时学习词向量的权重和Ngram概率模型的参数；

  03年论文中Bengio等人采用一个简单的前向反馈神经网络来拟合一个词序列的条件概率$p(w_t|w_1,w_2,...,w_{t-1})$，整个模型网络结构为一个三层神经网络，第一层为映射层，第二层为隐层，第三层为输出层。

![img](https://img2018.cnblogs.com/blog/1816627/201909/1816627-20190927110741486-399813391.png)

用端到端的思想来说，输入一个词的one-hot向量表征，希望得到相应词的条件概率，则神经网络模型要做的就是拟合一个由one-hot向量映射为相应概率模型的函数。上图的网络结构可以拆成两部分。

- 首先是一个线性的映射层。它将输入的N-1个one-hot词向量，通过一个共享的D$\times$V的矩阵C，映射为N-1个分布式的词向量。其中，V是词典的大小，D是embedding向量的维度。C矩阵里存储了要学习的word vector。

  <img src="https://img2018.cnblogs.com/blog/1816627/201909/1816627-20190927110755699-984011996.png" style="zoom:	50%;" />

这样，在映射过程中。映射层的权重就等效我们需要的词向量表征。

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220501184854858.png" style="zoom:40%;" />

- 其次是一个简单的前向反馈神经网络g，它由一个激活函数为tanh的隐层和一个Softmax输出层组成，可以将我们得到的一系列输出映成对应概率。这样，通过将Embedding层输出的N-1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做预估。
  $$
  p(w_i|w_1,w_2,...,w_{t-1})\approx f(w_i,w_t-1,...,w_{t-n+1})=g(w_i,C(w_{t-n+1}),...,C(w_{t-1}))
  $$

注意到，当词表长度和期望的词向量维度确定的时候，第一层映射层和softmax输出层的规模就已经确定了，而隐藏层的大小可以由我们自己指定。我们可以通过最小化一个带正则项的cross-entropy损失函数来调整神经网络的模型参数$\theta$：
$$
L(\theta)=\frac{1}{T}\sum_{t}logf(w_t,w_{t-1},...,w_{t-n+1})+R(\theta)
$$
上式的前半部分是输出概率的交叉熵，后半部分是包含映射层和隐藏层的神经网络参数的正则项，这将包含一个巨大的参数空间。不过，在每次用SGD学习更新模型的参数时，并不是所有的参数都会进行调整，只有当前context包含词的词向量才会被更新（因为映射层的输出只会得到这些词的输出，并参与接下来的运算）。真正的计算瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。

In all，这个模型解决了两个问题：

	1. 一个是统计模型里关注的条件概率$p(w_t|context)$的计算；
	2. 一个是向量空间模型里关注的词向量的表达；

这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。另一方面，以条件概率𝑝(𝑤𝑡|𝑐𝑜𝑛𝑡𝑒𝑥𝑡)p(wt|context)。为学习目标去更新词向量的权重，具有更强的导向性，同时也与Distributional Hypothesis不谋而合。

但NNLM模型仍然存在一系列问题：

1. 一个问题是，由于NNLM模型使用的是全连接神经网络，因此只能处理定长的序列。
2. 另一个问题是，由于其巨大的参数空间，将NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。

## 12.7 Word2Vec

Word2Vec总的来说包括“两个模型”+“两个提速手段”

### 12.7.1 两个模型

#### CBOW	

CBOW与之前的NNLM更为相似，即输入中间词前后共C个词，预测中间词，在这个过程中训练处需要的词向量矩阵。

<img src="https://img2018.cnblogs.com/blog/1816627/201909/1816627-20190927110908257-516031113.png" alt="img" style="zoom: 67%;" />

下面讲一下该模型结构：

- 图中$[x_{1k},...,x_{Ck}]$表示第 $k$个中心词的前后$C$个上下文的 one-hot 向量;
- 将one-hot向量输入存放词向量的矩阵$W_{V\times N}$进行查表，$V$为词表的大小，$N$为词向量的维度；
- 将查表得到的上下文向量直接求和，再通过一个$N\times V$的矩阵映射到输出层；

对比NNLM神经语言模型，它们主要有以下三个不同点：

​	1）移除NNLM模型的Hidden Layer结构；

​	2）直接将Embedding Layer的查表结果进行求和（NNLM是输出结果拼接）；

​	3）将上下文单词纳入上下文环境，真正考虑了Context（NNLM输入严格来说是上文文本）；

#### Skip-gram

从target-word对context的预测中学习word vector。

<img src="https://img2018.cnblogs.com/blog/1816627/201909/1816627-20190927112945641-539778791.png" alt="img" style="zoom:67%;" />

如果将skip-gram模型输出层的前向计算过程写成数学形式，可以表示为：
$$
P(w_0|w_i)=\frac{e^{U_0\cdot V_i}}{\sum_{j}e^{U_i\cdot V_i}}
$$
其中，$V_i$是embedding层矩阵的列向量，即中心词的词向量。$U_j$是SoftMax矩阵里的行向量，即上下文对应词的词向量。

skip-gram本质是计算输入词的input vector与目标word的output vector间的余弦相似度，并进行softmax归一化。

### 12.7.2 两个提速手段

普遍认为Hierarchical SoftMax对低频词效果较好；Negative Sampling对高频词效果较好，向量维度较低时效果更好。

#### **层次SoftMax**

简单来说，就是通过构造一个Huffman树，将复杂的归一化概率问题转化为一系列二分类的条件概率相乘的形式。

#### Huffman树

Huffman编码又称为最优二叉树，表示一种带权路径长度最短的二叉树。带权路径长度，指的就是叶子结点的权值乘以该结点到根结点的路径长度。而我们需要构造的Huffman树结构，是以词表为根结点，每一个子节点为父节点的不相交子集，词为叶节点的结构。我们将叶节点的权值转化为词频，则带权路径长度指的就是词频乘以路径的大小，带权路径最小的条件使得构造出来的霍夫曼树中，高频词离根结点更近，而低频词离根结点更远。其构造的Huffman树如下所示：

<img src="https://img2018.cnblogs.com/blog/1816627/201909/1816627-20190927112851732-342091968.jpg" alt="img" style="zoom:50%;" />

#### 目标词概率

![image-20220501201929367](/Users/lirui/Library/Application Support/typora-user-images/image-20220501201929367.png)

![image-20220501202151706](/Users/lirui/Library/Application Support/typora-user-images/image-20220501202151706.png)

![image-20220501202203429](/Users/lirui/Library/Application Support/typora-user-images/image-20220501202203429.png)

![image-20220501202213927](/Users/lirui/Library/Application Support/typora-user-images/image-20220501202213927.png)

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220501202232487.png" alt="image-20220501202232487" style="zoom:67%;" />

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220501202253589.png" alt="image-20220501202253589" style="zoom:67%;" />

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220501202315015.png" alt="image-20220501202315015" style="zoom:67%;" />

#### 负采样

与改造模型输出概率的Hierarchical Softmax算法不同，NCE算法改造的是模型的似然函数。其思想来源于一种叫做噪声对比估计（Noise-Contrastive Estimation）的算法。

以Skip-gram模型为例，其原始的似然函数对应着一个多项分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：
$$
J(\theta)=-\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j \le c,j\neq0}logp(w_{t+j}|w_j)
$$
式中的$p(w_{t+j}|w_t)$是整个词典归一化的概率。

在NCE算法中，我们构造了这样一个问题：对于一组训练样本，我们想知道，目标词的预测，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：
$$
p(D=1|w,context)=\frac{p(w|context)}{p(w|context)+kp_n(w)}=\sigma(logp(w|context))-logkp_n(w)
$$
![image-20220501203523635](/Users/lirui/Library/Application Support/typora-user-images/image-20220501203523635.png)

#### 总结

层次化softmax在一次forward+backward的过程中，只需要计算从根节点到ground truth label对应word这条路径上的概率，根据极大似然原理，使这条路径概率最大化，而不用每个叶子节点都去计算一遍从根节点到叶子节点路径的概率。和负采样一样，负采样是挑选N个单词作为负例，然后根据极大似然估计计算使得ground truth label对应的word成为正例的概率最大，并使得这N个单词成为负例的概率最大。

### 12.7.3 一些预处理细节

**低频词处理**
利用语料来建立词典的时候，不是每一个出现过词都会加入到词典中。通常会设置一个min_count的阈值参数，用来剔除出现次数少于min_count次的词。注意，这一类词在训练中也是不可见的，可以看作在训练之前就将其从语料库中删除了。

**高频词的处理**
通常认为高频词往往只提供较少的信息（比如“的”、“是”等高频副词对于名词没有很强的相关性的），而且这些词对应的词向量在众多样本的训练过程中也不会发生明显的变化。因此我们引入一种下采样（Subsampling）的操作，对一些出现频率高于一定值的高频词，按一定概率进行抛弃。这样的操作一方面可以提高我们的训练速度，另一方面也可以提升低频词的表示精度。

**单个句子的限制**
模型训练是以行进行的。通常我们存储语料的时候是以行为单位来存储句子，训练的时候每次就是处理一个句子。为了避免句子过长，通常可以设置一个阈值参数max_sentence_length，进行强行截断。

**对于context的选择**
对于一个给定行，假设其包含T个词，则该行依据每一个词及其对应的context，共可生成T个训练样本。实际上的context选取是这样的：事先设置好一个窗口阈值函数window，每次构造context的时候，首先生成区间[1, window]上的一个随机整数c，然后对当前中心词前后各取c个词构成context。

**自适应学习率**
在word2vec源码中，采用了自适应学习率：预先设置一个初始学习率$\eta_0$，每处理完一定数量的词进行一次调整，直到学习率到达一个下限值$\eta_{min}$，即训练过程中学习率是从$\eta_{0}$到$\eta_{min}$单调递减的，下降到$\eta_{min}$时维持在恒定值保持不变。

**训练**
模型采用的是随机梯度下降算法，且**仅对整个语料遍历一次**，这也是其高效的一个原因。

**多线程并行**
word2vec的训练是支持多线程的，可以通过设置num_threads来设置使用的线程数。

### 10.7.4 Word2Vec的局限性

总的来说，word2vec通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重，而这个过程可以看作是**无监督**或称为**自监督**的，其词向量的训练结果与语料库是紧密相关的，因此通常不同的应用场景需要用该场景下的语料库去训练词向量才能在下游任务中获得最好的效果。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中，从此之后不单单是词向量，我们也有了句向量、文档向量，从Word Embedding走向了World Embedding的新时代。word2vec非常经典，但也有其明显的局限性，其主要在以下几个方面：

1. 在模型训练的过程中仅仅考虑context中的局部语料，没有考虑到全局信息；

2. 对于英文语料，对于什么是词，怎样分词并不是问题（但个词就是独立的个体）。而对于中文而言，我们在训练词向量之前首先要解决分词的问题，而分词的效果在很多情况下将会严重影响词向量的质量（如分词粒度等），因此，从某些方面来说word2vec对中文不是那么的友好；

3. 在2018年以前，对于word2vec及其一系列其他的词向量模型都有一个相同的特点：其embedding矩阵在训练完成后便已经是固定了的，这样我们可以轻易从网上获取到大量预训练好的词向量并快速应用到我们自己任务中。但从另一个角度来说，对于同一个词，在任意一个句子，任意一个环境下的词向量都是固定的，这对于一些歧义词来说是存在较大问题的，这也是限制类似word2vec、Glove等词向量工具性能的一个很重要的问题。

   [word2vec 霍夫曼树](https://www.cnblogs.com/rossiXYZ/p/13427829.html)

## 12.8 Glove

### 12.8.1 统计共现矩阵

在介绍GloVe的思想之前，我们先定义一个共现矩阵X，该矩阵中的$X_{ij}$表示第𝑗个单词出现在以第𝑖个单词为中心，长度为𝑛的窗口中的次数。将长度为n的窗口遍历整个语料库，则得到了共现矩阵𝑋。

### 12.8.2 Glove的由来

对于大型语料库，我们可以认为统计的词共现矩阵X可以很好的描述词与词之间的相关性。但是，就像之前我们说过的，这样大的一个共现矩阵在实际使用中将会面临复杂的维度灾难问题，因此我们需要想办法词向量进行降维，比如之前的SVD对词-文档共现矩阵进行降维就是这样一种思想。

而对于word2vec，我们每次训练词向量，都是针对于局部语料进行预测（根据局部上下文预测中心词，或根据中心词预测局部上下文），这就使得模型的训练过程中是很难考虑到整个语料库的全局信息的。我们通常将其称为一种预测模型（predictive model），其目标是不断提高对其他词的预测能力，即减小预测损失，从而得到词向量。

我们有没有一种方法，既能通过训练的方式得到固定维度的词向量表征，又能够使得到的词向量能够充分考虑到语料库的全局特征？这样我们便得到了GloVe。

简单来说，Glove相对于Word2Vec，需要提前统计词共现矩阵，并将其整合到代价函数之中，使得训练结果对于该统计是有一定的重建能力的。我们将其称为一种统计模型（count-based model），其目标是优化减小重建损失（reconstruction loss），即降维之后的向量能尽量表达原始向量的完整信息。

### 12.8.3 代价函数

![image-20220501205034158](/Users/lirui/Library/Application Support/typora-user-images/image-20220501205034158.png)

![image-20220501205339810](/Users/lirui/Library/Application Support/typora-user-images/image-20220501205339810.png)

![image-20220501205402285](/Users/lirui/Library/Application Support/typora-user-images/image-20220501205402285.png)

![image-20220501205414021](/Users/lirui/Library/Application Support/typora-user-images/image-20220501205414021.png)

### 12.8.4 Glove、Word2Vec和LSA的区别

1）Glove和LSA的区别

-   Glove可以看做是对LSA的一个高效的矩阵分解算法，采用Adagrad对最小平方损失进行优化；

-  LSA基于共现矩阵构建词向量，实质是基于全局语料采用SVD进行矩阵分解；

2）Word2Vec和Glove

-   Word2vec是局部语料训练的，特征是基于划窗的，而glove的划窗是为了构建共现矩阵，是基于全局语料的，因此glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove需要固定语料信息；
-   Word2vec属于自监督学习，glove虽然被认为是无监督学习，但实际上还是有标签的，标签是共现次数$log(X_{ij})$；
-  Word2vec的损失函数实质是带权重的交叉熵，权重固定，Glove的损失函数是最小平方损失，权重可以做映射变换；
-  总体来看，Glove可以看做是更换了目标函数和权重函数的全局Word2Vec；

## 12.9 Transformer

1）组成

Encode+Decoder，其中encoder模块由self-attention模块和feed-forward模块构成，decoder由self-attention、feed-forward和encoder-decoder attention交互模块构成。

对于decoder的第一个模块，训练和测试时接收的输入不一样。

训练时，输入每个词的embedding，然后在多头注意力模块中做mask操作。

推理时，先生成第一个位置的输出，第二次预测时，将其加入到序列中，做递归预测。

2）多头encoder-decoder交互模块，Q/K/V形式及注意力计算方式与self-attention一致。只不过K/V来自于编码器的输出，Q来自解码器的输出。

3）Add-Norm模块，接在编码器和解码器每个子模块的输出后，Add表示残差连接，Norm表示layer normalization；

4）positional-encoding添加到encoder和decoder的输入端，和输入词的embedding求和；

​	采用正弦函数编码，可以让模型关注相对位置信息。
$$
\overrightarrow{p}_t^{(i)}=f(t)^{(i)}=\left\{\begin{matrix}
  sin(w_k\cdot t), i=2k& \\
  cos(w_k \cdot t), i=2k+1&
\end{matrix}\right.
$$

$$
w_k=\frac{1}{10000^{2k/d}}
$$

对于任何固定的偏移量k，$PE_{pos+k}$可以表示$PE_{pos}$的线性函数；

5）scaled-dot product

随着$d_k$的增大，$q*k$点击后的结果也随之增大，SoftMax可能落入梯度比较小的区域，因此收敛困难，除以$d_k$使attention权重分布方差为1。

6）attention原理

目标字做query，上下文做key，q、k计算相似性后与value相乘，加权融合。

## 12.10 ELMO

![image-20220501232426553](/Users/lirui/Library/Application Support/typora-user-images/image-20220501232426553.png)

模型通过上游的语言模型预训练，学习出三个特征向量，分布表示单词特征、句法特征和语义特征，在送入下游任务进行学习。

## 12.11 GPT

用Transformer的解码器代替了RNN，预训练任务为text prediction和task classifier。

![image-20220501233055482](/Users/lirui/Library/Application Support/typora-user-images/image-20220501233055482.png)

与ELMO只提供预训练好的项目不同，这里预训练的模型也一同提供给下游任务

## 12.12 Bert

### 12.12.1 特点 

- 使用Transformer为主要框架，可以捕捉语句中的双向关系；
- 使用Masked Language Model（MLM）和Next Sentence Prediction（NSP）的多任务训练目标；

优势：Bert表征会基于所有层中的左右两侧语境。

### 12.12.2 输入表示 

BERT的输入的编码向量是三个嵌入特征的单位和；

1）WordPiece嵌入：WordPiece是指单词划分为一组有限的公共字词单元，能在单词的有效性和字符串的灵活性间取得一个折中的平衡；

2）位置嵌入：加入可学习的位置编码向量；

3）分隔嵌入：用于区分两个句子，例如B是否是A的下文，对于句子对，第一个句子的特征值为0，第二个句子的特征值为1。

### 12.12.3 预训练 

1）MLM：训练的时候随机从语料中MASK掉一些词，然后通过上下文预测这些词；

- 15%的wordpiece token会被随机MASK掉

- 在确定要mask的词后，80%的时候会被直接替换为[MASK]，10%替换为其他单词，10%保留原始token；

2）NSP：判断句子B是否为句子A的下文，如果是，输出"isNext"，如果不是，则输出"NotNext"；

- 训练数据随机抽取连续的两句话，50%保留被抽取的两句话，符合isNEXT关系，另外50%的第二句话是随机从语料中提取的，关系是NotNext，关系保存在[CLS]中；

# 13 多任务建模

## 13.1 share-bottom

- 多个任务公用一个embedding层(f(x))，k个子任务分别对应一个tower network（$h^k$），每个子任务的输出为$y_k=h^k(f(x))$

## 13.2 MMOE

- N个专家网络代替了share-bottem结构，k个gate网络为k个子任务服务，expert网络是所有任务共享的。
  $$
  y_k = h^k(f^k(x)),f^k(x)=\sum_{i=1}^ng^k(x)_if_i(x)
  $$

  - 其中，$g^k(x)$为SoftMax($W_{gk}x$)，输出是所有expert上的权重，gate网络和expert网络的实现都依靠全连接。

- MMOE的multi-gate结构对于缓解不同任务差异的冲突有一定的缓解作用。

- 可以通过皮尔逊系数判断任务间的相关性，任务相关度低时，MMOE的结构通常会好些。

## 13.3 ESSM

- Motivation
  - 样本选择偏差（训练样本与推理样本的解空间不一致，训练样本的解空间是推理空间的子集，因此有分布偏移问题）；
  - 数据稀疏问题；
- CVR预估的本质：假设样本被点击，那么转换的概率是多少？

$$
p(z\&y=1|x)=p(z=1|y=1,x)p(y=1|x) 
$$

​			其中，$p(z\&y=1|x)$为pCTCVR，$p(z=1|y=1,x)$为pCVR，$p(y=1|x)$为pCTR。

- 隐式学习CVR，联合优化CTR和CTCVR
  $$
  L(\theta_{cvr},\theta_{ctr})=\sum_{i=1}^Nl(y_i,f(x_i;\theta_{ctr}))+\sum_{i=1}^Nl(y_i\&z_i, f(x_i;\theta_{ctr})*f(x_i;\theta_{cvr}))
  $$
  
- 对于CTR任务，正样本为曝光且点击，负样本为曝光未点；对于CTCVR任务，正样本为曝光点击且转化，负样本为曝光未点击+曝光点击未转化；

# 14 知识图谱

​		知识图谱本质上，是一种揭示实体之间关系的语义网络。它由一条条知识构成，每条知识都可以表示为一个三元组，就是头实体-关系-尾实体。在逻辑结构上可分为模式层与数据层两个层次，数据层由一系列知识三元组组成，模式层相当于知识的一个概念模板，用来规范化数据层知识表达。大规模的知识图谱构建需要用到多种技术，比如说知识抽取，是从一些结构化、非结构化的数据中提取实体关系属性等；知识融合，可以消除抽取的实体和事实指代对象之间的歧义，提高知识图谱的质量，知识推理可以在已有的知识库里面进一步挖掘隐含的知识，从而丰富和扩展它的内容。另外知识表示技术上的进展也对知识图谱的构建、推理以及应用起到了重要的作用，比如图嵌入学习就是一种知识表示学习，常见的方法有DeepWalk、node2vec，还有Trans系列模型，TransE，TransR，TransH等等。构建知识图谱的目的，就是对互联网大规模的数据组织、管理以及利用提供一个更有效地方式，它可以促进例如智能搜索，深度问答等等。把知识图谱应用到推荐领域，一方面是可以缓解以前个性化推荐系统面临的数据稀疏和冷启动问题，一方面也可以赋予推荐结果一定的可解释性，所以结合知识图谱的推荐也是近年来一个重要的方向。

1）TransE：

将关系视为低维向量空间中头实体到尾实体的翻译操作，即h+r=t；

2）TransH：

- 解决TransE不能很好地处理1-n、n-1及n-n这样的复杂关系，将h和t投影到r所在的平面上，然后用投影后的头尾实体计算三元组得分；

- 提出了新的负采样方法bern；

  ![image-20220503001706659](/Users/lirui/Library/Application Support/typora-user-images/image-20220503001706659.png)

3）TransR

将TransR的投影到超平面更进一步，投影到一个空间上

![image-20220503001747241](/Users/lirui/Library/Application Support/typora-user-images/image-20220503001747241.png)

# 15 向量近邻查找

## 15.1 KDTree

二叉树，核心思想是对k维特征空间不断以中值递归划分构造树，每一个节点是一个超矩形，小于该节点的样本被划分到左子树，大于该节点的样本被划分到右子树。

检索时：
 1）从根节点出发，递归地向下访问kd树，直到到达叶子节点

2）以叶节点为最近点，向上回退，每次回退时比较一下兄弟节点中是否存在更近的点，若存在则更新最近点，否则继续回退；

Kd树一般用于训练样本数远大于空间维数时的k近邻搜索；

KD树的搜索复杂度为O(logN)

## 15.2 BallTree

BallTree在一系列嵌套的超球面上分隔数据

## 15.3 Annoy

通过建立一个二叉树来使得每个点的查找的时间复杂度是O(logn),和kd树不同，annoy没有对k维特征进行划分，Annoy的每一次空间划分，可以看做聚类数为2的kmeans过程。

![image-20220504171215409](/Users/lirui/Library/Application Support/typora-user-images/image-20220504171215409.png)

# 16 卷积神经网络

## 16.1 1x1卷积的作用

- 可以实现跨通道的交互和信息整合；
- 实现卷积核通道数的降维和升维；
- 可以实现与全连接层等价的效果；

## 16.2 CNN的特点与优势

- 局部连接；
- 参数共享，提升模型平移不变性；

## 16.3 pooling层反向传播时怎么处理

- max-pooling：求导时，只需保留前向传播中已经被选择的节点的值，其他值为0；
- mean-pooling：反向传播时，需要将值平均分配给每一个神经元在进行反向传播；

## 16.4  SENet

Senet希望模型能关注到channel之间的关系，希望模型可以自动学习不同channel间的重要程度

![image-20220504174808185](/Users/lirui/Library/Application Support/typora-user-images/image-20220504174808185.png)

Senet首先对特征图作squeeze操作，得到channel级别的全局特征（将一个channel上整个空间特征编码为一个全局特征，采用global avg pooling实现。然后对全局特征进行excitation操作（sigmoid），学习不同channel间的关系，得到各个channel间的权重，最后乘以原来特征图得到最终特征。本质上，channel是在各个channel特征维度做attention或gating操作，这种注意力机制模型可以帮助模型关注信息量更大的channel特征，抑制那些不重要的特征。

## 16.5  Resnet

1）抑制网络退化，resblock基于一个假设，即当把浅层网络特征恒等映射至深层网络时，深层网络的效果一定比浅层网络好，有点类似于特征融合（浅层网络学习全局特征，深层网络学习纹理等更细节的特征）。因此resnet构造了一个恒等映射，增加一个跳过连接，让前面的信息能够更顺畅的传递；

2）梯度项有个常数，保证梯度顺畅回传，缓解梯度消失；

# 17 循环神经网络RNN、LSTM和GRU

## 17.1 RNN

### 17.1.1 RNN的结构

RNN在t时刻的神经元的输入包括：当前时刻的输入$x_t$以及上一时刻的隐层状态$h_{t-1}$，输出包括：当前时刻的隐层状态$h_t$和当前时刻的输出$y_t$。

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504211246099.png" alt="image-20220504211246099" style="zoom: 33%;" />

RNN中的输入$x_t$只包括t时刻的信息，不包含顺序信息；而$h_t$是根据$x_t$和$h_{t-1}$计算得到的，包含了历史输入信息与当前输入信息。计算$h_t$时激活函数采用Tanh，计算输出$y_t$时采用SoftMax
$$
h_t=\sigma(z_t)=\sigma(Ux_t+Wh_{t-1}+b)\\
y_t=\sigma(Vh_t+c)
$$

### 17.1.2 RNN缺陷

假设一个只有三个输入数据的序列：
$$
h_1= Tanh(Ux_1+Wh_0+b)\\
h_2= Tanh(Ux_2+Wh_1+b)\\
h_3= Tanh(Ux_3+Wh_2+b)\\
y_1=SoftMax(Vh_1+c)\\
y_2=SoftMax(Vh_2+c)\\
y_3=SoftMax(Vh_3+c)\\
$$
RNN在t时刻的损失函数为$L_t$，总损失函数是$L=L_1+L_2+L_3$

t=3时刻的损失函数$L_3$对于网络参数U、V、W的梯度如下：

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504213301955.png" alt="image-20220504213301955" style="zoom: 33%;" />

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504213630608.png" alt="image-20220504213630608" style="zoom:33%;" />

## 17.2 LSTM

### 17.2.1 LSTM的结构

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504214253074.png" alt="image-20220504214253074" style="zoom:33%;" />



LSTM神经元在RNN的基础上增加了cell状态$c_{t-1}$，保存了历史的信息，LSTM内部还包含了输入门、遗忘门和输出门。、
$$
f_t=Sigmoid(W_f[h_{t-1}, x_t]+b_f)\\
c'_{t-1}= c_{t-1}\odot f_t
$$
**遗忘门**：遗忘门用来判断cell状态$c_{t-1}$的哪些信息应该被删除。其中 **σ** 表示激活函数 sigmoid。输入的$h_{t-1}$和 $x_t$经过 sigmoid 激活函数之后得到 $f_t$，$f_t$中每一个值的范围都是 [0, 1]。$f_t$ 中的值越接近 1，表示 cell 状态$c_{t-1}$中对应位置的值更应该记住；$f_t$中的值越接近 0，表示 cell 状态 $c_{t-1}$中对应位置的值更应该忘记。将$f_t$与$c_{t-1}$按位相乘 (ElementWise 相乘)，即可以得到遗忘无用信息之后的$c'_{t-1}$。
$$
i_t=Sigmoid(W_i[h_{t-1}, x_t]+b_i)\\
\tilde{C}_t=Tanh(W_C[h_{y-1}, x_t]+b_c)\\
c_t= c'_{t-1}+(i_t\odot \tilde{C}_t)
$$
**输入门**：用来判断哪些新的信息应该加入到 cell 状态 **c**'t-1 中。其中 **σ** 表示激活函数 sigmoid。输入的 **h**t-1 和 **x**t 经过 tanh 激活函数可以得到新的输入信息 (图中带波浪线的 **C**t)，但是这些新信息并不全是有用的，因此需要使用 **h**t-1 和 **x**t 经过 sigmoid 函数得到 **i**t， **i**t 表示哪些新信息是有用的。两向量相乘后的结果加到 **c**'t-1 中，即得到 t 时刻的 cell 状态 **c**t。
$$
o_t=Sigmoid(W_o[h_{t-1},x_t]+b_o)\\
h_t=o_t\odot \tilde{C}_t
$$
**输出门**：用来判断应该输出哪些信息到 **h**t 中。cell 状态 **c**t 经过 tanh 函数得到可以输出的信息，然后 **h**t-1 和 **x**t 经过 sigmoid 函数得到一个向量 **o**t，**o**t 的每一维的范围都是 [0, 1]，表示哪些位置的输出应该去掉，哪些应该保留。两向量相乘后的结果就是最终的 **h**t。

### 17.2.2 LSTM缓解梯度消失、梯度爆炸

RNN出现梯度消失的原因是梯度函数中包含一个连乘项，LSTM通过门的作用，可以使连乘项约等于0或者1，

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504220222205.png" alt="image-20220504220222205" style="zoom:33%;" />

## 17.3 GRU

GRU相比LSTM，只有两个门，即更新门和重置门，另外，GRU没有LSTM中的细胞状态。

<img src="/Users/lirui/Library/Application Support/typora-user-images/image-20220504220350169.png" alt="image-20220504220350169" style="zoom: 33%;" />

# 18 Attention机制











